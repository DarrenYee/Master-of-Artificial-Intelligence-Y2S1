{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "10bbe6360f9659217f9f40455301aa72d6b614a6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yy-1P201ARs",
    "outputId": "25de93d2-59d4-4d46-aa43-c770bbf5220a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: nltk in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U pip\n",
    "!pip install -U dill\n",
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65559326e76d2b7841244e1dd6ea448671411b95",
    "id": "9wYhyO0h1ARv"
   },
   "source": [
    "# N-grams Language Models (N-grams LM)\n",
    "\n",
    "Nowadays, everything seems to be going neural... \n",
    "\n",
    "Traditionally, we can use n-grams to generate language models to predict which word comes next given a history of words. \n",
    "\n",
    "We'll use the `lm` module in `nltk` to get a sense of how non-neural language modelling is done.\n",
    "\n",
    "(**Source:** The content in this notebook is largely based on [language model tutorial in NLTK documentation by Ilia Kurenkov](https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1b213fb0dd5534ce82d6f1c716e9695ba5cf9758",
    "id": "AAgKZbKS1ARw"
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "800f7632b6a834d36b95211a912b390fcb9dc11b",
    "id": "7Bjf9G1e1ARx"
   },
   "source": [
    "If we want to train a bigram model, we need to turn this text into bigrams. Here's what the first sentence of our text would look like if we use the `ngrams` function from NLTK for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1c1716261478af5e75bba7799bfefc10bd1ea4ea",
    "id": "SSE_OgLj1ARx"
   },
   "outputs": [],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a5a1c775bab7cf9c67656d4349d8bfca02a80738",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cmWuwt01ARy",
    "outputId": "2b96f406-2cfb-428d-9a08-1837573b6b58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('b', 'c')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0219988318e39dd0576913b8087af8b35cbdab2f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3JdfRRF1ARy",
    "outputId": "80a689c2-ff4b-4163-bf55-53a6593253bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text[1], n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3a92b25dc25f3ae86fd265880b7f410e981c670",
    "id": "deR8lnqo1ARz"
   },
   "source": [
    "Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n",
    "\n",
    "Wouldn't it be nice to somehow indicate how often sentences start with \"a\" and end with \"c\"?\n",
    "\n",
    "\n",
    "A standard way to deal with this is to add special \"padding\" symbols to the sentence before splitting it into ngrams. Fortunately, NLTK also has a function for that, let's see what it does to the first sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "1ec58c94f58429c9160b4496c939f211f88ade54",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjFxQ8AS1ARz",
    "outputId": "0a81079d-94e5-425e-9742-d98bdb6773ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=2)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ae38644238aa46ac100381c0de046972090b093b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OykW8Bd1AR0",
    "outputId": "9394aefd-d7bb-4ae4-ff5e-8edb0dd2b50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "list(ngrams(padded_sent, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4cda3fcbd1b8ab48431240e04168011953ebc913",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoQ4PpCJ1AR1",
    "outputId": "7d7ecaba-1f12-4088-d2e3-efef89aa3f26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "bd572a57371a716471d156144e7ebc136072de01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3YfJEyL1AR1",
    "outputId": "87856431-20c8-419d-b3f4-97c6a204e873"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'a'),\n",
       " ('<s>', 'a', 'b'),\n",
       " ('a', 'b', 'c'),\n",
       " ('b', 'c', '</s>'),\n",
       " ('c', '</s>', '</s>')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=3))\n",
    "list(ngrams(padded_sent, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4d80e5707a2efa7ea5cee601d07bca23bc0ae87",
    "id": "R8L7ZUpD1AR1"
   },
   "source": [
    "Note the `n` argument, that tells the function we need padding for bigrams.\n",
    "\n",
    "Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway.\n",
    "\n",
    "Thus the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "eb64c3c2a205c5fc5e1fce6f63c3e9038f0d8c4a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQ5MoGSf1AR2",
    "outputId": "7bf3c07d-981e-4cef-e3bf-a96a21b0549e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(pad_both_ends(text[0], n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d2581fae73d38d623a77b5d523c957df2dc8478",
    "id": "zY-_plK81AR2"
   },
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1168917cc340d400f6dba9b2703561785481d8e4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0eQ6tws1AR3",
    "outputId": "9143c72b-23e5-4337-ab3a-1bc5945a7b49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(pad_both_ends(text[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2778a5e5b8d95bc395ba41af491818140c7f680e",
    "id": "RNKQR6Po1AR3"
   },
   "source": [
    "To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information.\n",
    "NLTK once again helpfully provides a function called `everygrams`.\n",
    "\n",
    "While not the most efficient, it is conceptually simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dfdc9effca718a7881f1fdc4562cdfb29164b96c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90vZbBk61AR3",
    "outputId": "e6fe2235-60c1-44c7-f996-68aa38bb3edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'a'),\n",
       " ('a',),\n",
       " ('a', 'b'),\n",
       " ('b',),\n",
       " ('b', 'c'),\n",
       " ('c',),\n",
       " ('c', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d74b0ba683b733d5a83756361a905625a841e68d",
    "id": "w_mmdZz_1AR4"
   },
   "source": [
    "We are almost ready to start counting ngrams, just one more step left.\n",
    "\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model.\n",
    "\n",
    "To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "310c937ea7cc847a26f6c3d08c7169e8cd6b354a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgNuuiuf1AR4",
    "outputId": "9405ae4b-80c2-43e9-e4bd-9c588d87d580"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bab7d656c474d945669e6a86bf4beec3a44189e",
    "id": "dDXnc34Q1AR4"
   },
   "source": [
    "In most cases we want to use the same text as the source for both vocabulary and ngram counts.\n",
    "\n",
    "Now that we understand what this means for our preprocessing, we can simply import a function that does everything for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "147ccdd961ac97492ee9f0eadf3dfce7325c6790",
    "id": "KH9p-U8r1AR4"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1b15504f2fb4487deec19de1c5ca786e49a42d6",
    "id": "cgZXRuKe1AR4"
   },
   "source": [
    "So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy iterators. They are evaluated on demand at training time.\n",
    "\n",
    "For the sake of understanding the output of `padded_everygram_pipeline`, we'll \"materialize\" the lazy iterators by casting them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "aa18a8a4a700926c30488c94cf519d8900de8ddf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohFgFDBN1AR5",
    "outputId": "f5e018f8-00fe-42ff-cd10-d49d034e760c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',), ('c', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'c'), ('c',), ('c', 'd'), ('d',), ('d', 'c'), ('c',), ('c', 'e'), ('e',), ('e', 'f'), ('f',), ('f', '</s>'), ('</s>',)]\n",
      "\n",
      "#############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('#############')\n",
    "list(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4638a871a58694a77473a590728ebf25047cd6eb",
    "id": "FV6N4GCY1AR5"
   },
   "source": [
    "## Lets get some real data and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c679084566e711286d861a622e21ca5c04db7bee",
    "id": "mVnQOAnw1AR5"
   },
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9bb181f26fb777464b7d4e6d08beac724864cefe",
    "id": "vFbwg0dE1AR5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "af4102fcbf643e09319ca6394c175ca64082fd19",
    "id": "_ZAxixiR1AR6"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9daa634f65b63882f7192a9489fba198970f3771",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkU2ipFo1AR6",
    "outputId": "07b03a06-07e7-4024-ba9a-f85d7d6d486b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "91c93337cfea53b0c15a6e8617395dab872fb140",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMyoZy1_1AR6",
    "outputId": "7c504b1e-c68f-4fa5-f619-cadf7303fc5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish \n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3eef00b8b1a95043e1c8bdd6912ae22060198e5c",
    "id": "2laPwgfh1AR6"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32c5ad89e3e61b143e940d50a0d0ee602dadfe3f",
    "id": "fZixcnHv1AR6"
   },
   "source": [
    "# Training an N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1ff506e3a14df8283cb93b0a45f86862d3e3c7",
    "id": "aKFEsiY91AR6"
   },
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
    "\n",
    "We only need to specify the highest ngram order to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "2635458b0f3fed618bfae6a0705ccb11555bfb0b",
    "id": "VDk07coK1AR6"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69cdd0e5babaf059a7b9bc87dc0a6e261cef6cb2",
    "id": "51MW_W5W1AR7"
   },
   "source": [
    "Initializing the MLE model, creates an empty vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "93b95f89f020585dabfbcd6c41273c823a91e882",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7x3rPwWQ1AR7",
    "outputId": "aa95e6e0-6a20-45f3-d3aa-f87e8c45c7b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb567e332a0089c86e388a8c93f32d1a31737a29",
    "id": "b1puajAs1AR7"
   },
   "source": [
    "... which gets filled as we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "a31ecc7f6df30a42df9ed6a79039b50f734299cf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGUHacjn1AR7",
    "outputId": "2a0fb58c-3572-4f4a-ed61-b57c584320d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 0 items>\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "6fd7e68c2cdacbe1901fd3b8c05c440706d19100",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRzj5ZAc1AR7",
    "outputId": "094c0d87-3ed1-4ff6-e4d1-30dfd8149063"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74622c58a2df2ac08c8b0c79644d1f87f4014f44",
    "id": "ZNRqqjZ31AR8"
   },
   "source": [
    "The vocabulary helps us handle words that have not occurred during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "e5670e34f9d1950f6a1515d57db18092d39484f2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6T8r5kp1AR8",
    "outputId": "36873a68-3ed3-448a-c0d6-eacaff123577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "cfec70768e8976073dda05f702f3f210a33dc1c3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thHDliXS1AR8",
    "outputId": "b8844056-002c-4863-c9e2-63bd77ded17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('language is never random lah .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83a71e4b25b53795dedce91c1a5686b7e22279e0",
    "id": "k2zB4tBu1AR8"
   },
   "source": [
    "Moreover, in some cases we want to ignore words that we did see during training but that didn't occur frequently enough, to provide us useful information. \n",
    "\n",
    "You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, To find out how that works, check out the docs for the [`nltk.lm.vocabulary.Vocabulary` class](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0310a2f3c7d9e90f4574b44838336ca1e4ef2ef",
    "id": "fOciOqe91AR8"
   },
   "source": [
    "**Note:** For more sophisticated ngram models, take a look at [these objects from `nltk.lm.models`](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af5a0851a2f8707ea2e172681342ed3ecd872328",
    "id": "G5GJOT471AR8"
   },
   "source": [
    "# Using the N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d098fd4686bcdc0ab9aa72dbbd4b62a17ee2c332",
    "id": "iO-jd8qL1AR8"
   },
   "source": [
    "When it comes to ngram models the training boils down to counting up the ngrams from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "bfc60d61539269298390044c0d3415bfd28e2b1e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cQR6ngA1AR9",
    "outputId": "43e286e9-e6c0-4051-ec22-43e6df376bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 1 ngram orders and 0 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9b8cac68de80aee9c7cf7b7f2faf2b199ad27cc",
    "id": "lfUCIRM-1AR9"
   },
   "source": [
    "This provides a convenient interface to access counts for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "90f4611580c41118747a42cfac4c9d729f02523f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkqnmMFb1AR9",
    "outputId": "ebef95f0-4178-4d3c-a98c-189040739b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts['language'] # i.e. Count('language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccdf1bf83ba16f0a43f04eb96ca224a968cc81d2",
    "id": "5-rE271s1AR9"
   },
   "source": [
    "...and bigrams for the phrase \"language is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "3ba55a83d7c052d1f14923c4046a5f6c86d72c8c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtDan1351AR9",
    "outputId": "80ca7af6-98e0-468a-a7c2-50e481e560ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language']]['is'] # i.e. Count('is'|'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bceda0a01e9f2642dd30a6b341c24a606720b9",
    "id": "gxOqnPTw1AR9"
   },
   "source": [
    "... and trigrams for the phrase \"language is never\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "813e30e1fb153002cfb020d430def2510bf43bbc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3EK4vsw1AR_",
    "outputId": "8ff579f1-6082-4228-dccb-82c8f5fbfd4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c4338d351cfc3cd785673ad9581ecab161b24c6",
    "id": "_ZYVT9N-1AR_"
   },
   "source": [
    "And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts.\n",
    "\n",
    "This being MLE, the model returns the item's relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "051ef5a06c004a8e8ddb6168ca318bc8c0c9abf4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh_tqheb1AR_",
    "outputId": "3d2506ac-045e-4692-999f-8b13c1481fd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('language') # P('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "ab3d48919b8f074342e624a1da441621829a4dbd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0rO7xKe1AR_",
    "outputId": "bf7614b6-ab46-4581-8a4b-f5ea3464b769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('is', 'language'.split())  # P('is'|'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "73955908d499f459e19c2cbfaec7d94c513acb91",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTSoiNHy1ASA",
    "outputId": "31cad60e-c8ff-446c-e1e9-b57a41d419d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('never', 'language is'.split())  # P('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946dde75e8d5b8d8878271359b0d91926abeb199",
    "id": "pVdcyZ3J1ASA"
   },
   "source": [
    "Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token.  This is \"<UNK>\" by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "44038e8cd8d078b734a6b1a803d0795517e2fa82",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZoBMonf1ASA",
    "outputId": "e709c57f-9260-4495-8d27-578c800458f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "f2f170a0bbca4b33b5eb1a55a55f0e1b1b18a634",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A48tWGzc1ASA",
    "outputId": "e8223043-dec0-43ff-e2c0-c22847163793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"leh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "9107d1a7c27f30a449e9f370002a6406d5a1396d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6ONGLrJ1ASA",
    "outputId": "99158056-ff71-4210-e396-56369ae9514f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a9ad0be90c72ece6c2a1c00314ea60f41a00f81",
    "id": "tR7OHDxE1ASB"
   },
   "source": [
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "\n",
    "For convenience this can be done with the `logscore` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "20ef81b7e161df3b9ab27236c33559713b3077ce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogxubiMT1ASB",
    "outputId": "ec6d48f3-9432-4f19-b5c4-39fa3597b462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logscore(\"never\", \"language is\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f9e0a8d0aba302532b8a84f342d1bf4d3e202a",
    "id": "ZTBEPhiA1ASB"
   },
   "source": [
    "# Generation using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f893159fe093aef484c07b37d922de4ac5727834",
    "id": "49VmWoXA1ASB"
   },
   "source": [
    "One cool feature of ngram models is that they can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "98989ec4bae592fc98332e759daf9e42bac4213e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LN4i1BR1ASB",
    "outputId": "ab8a4db8-8427-42fd-b7e8-270da63c55c2"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m20\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b8d07eaf3afae978131573ae127fa0ec3f2e50d",
    "id": "husgLG1h1ASB"
   },
   "source": [
    "We can do some cleaning to the generated tokens to make it human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "73d0d5e0029e64876100e0f2e368b4835a99efcd",
    "id": "dZ_dNbdQ1ASB"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "949378240cbc8247579b79946b113a3afd039b39",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "s6dRBpg81ASB",
    "outputId": "f2811531-f9fc-46e8-92e1-f605d8639a79"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_sent(model, \u001b[38;5;241m20\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_sent\u001b[1;34m(model, num_words, random_seed)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param model: An ngram language model from `nltk.lm.model`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param num_words: Max no. of words to generate.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param random_seed: Seed value for random.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(num_words, random_seed\u001b[38;5;241m=\u001b[39mrandom_seed):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "9041c514e0458236132fe9b3c42ac2ce651beb92",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbTTMqI61ASC",
    "outputId": "32a7788f-75cc-405e-8db8-fcef2a14b258"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m28\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "print(model.generate(28, random_seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uG6qptsQ1ASC",
    "outputId": "21a23a95-c706-4aec-8b6e-fdcc297ab313"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_sent(model, \u001b[38;5;241m28\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_sent\u001b[1;34m(model, num_words, random_seed)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param model: An ngram language model from `nltk.lm.model`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param num_words: Max no. of words to generate.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param random_seed: Seed value for random.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(num_words, random_seed\u001b[38;5;241m=\u001b[39mrandom_seed):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "generate_sent(model, 28, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "bce2885bd22d0bb1f79c3861f10fde7df6714e40",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "D4faj0Rs1ASC",
    "outputId": "904fcc86-291b-4c64-b185-b41297f3483f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_sent(model, \u001b[38;5;241m20\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_sent\u001b[1;34m(model, num_words, random_seed)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param model: An ngram language model from `nltk.lm.model`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param num_words: Max no. of words to generate.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param random_seed: Seed value for random.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(num_words, random_seed\u001b[38;5;241m=\u001b[39mrandom_seed):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "90ddb0e7d0fb52bc77d08425331944c55ee0885e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Czs6eId41ASC",
    "outputId": "3db26799-03d7-401c-fa34-b78387b51895"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_sent(model, \u001b[38;5;241m20\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_sent\u001b[1;34m(model, num_words, random_seed)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param model: An ngram language model from `nltk.lm.model`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param num_words: Max no. of words to generate.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param random_seed: Seed value for random.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(num_words, random_seed\u001b[38;5;241m=\u001b[39mrandom_seed):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "ba9546c6800d9e0b0f0af4f0fd31a141af93b12f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2PBUnOxj1ASC",
    "outputId": "fe495853-1cc0-4558-9497-572b8aed8f69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0iIjvHJ1ASC"
   },
   "source": [
    "# Saving the model \n",
    "\n",
    "The native Python's pickle may not save the lambda functions in the  model, so we can use the `dill` library in place of pickle to save and load the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ekarloLT1ASC"
   },
   "outputs": [],
   "source": [
    "import dill as pickle \n",
    "\n",
    "with open('kilgariff_ngram_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "33Cr4MTL1ASD"
   },
   "outputs": [],
   "source": [
    "with open('kilgariff_ngram_model.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bhG8whoN1ASD",
    "outputId": "cdc0166a-4dec-4da6-f60c-fb0d6e3ceb60"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't choose from empty population",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_sent(model_loaded, \u001b[38;5;241m20\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_sent\u001b[1;34m(model, num_words, random_seed)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param model: An ngram language model from `nltk.lm.model`.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param num_words: Max no. of words to generate.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param random_seed: Seed value for random.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgenerate(num_words, random_seed\u001b[38;5;241m=\u001b[39mrandom_seed):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[38;5;241m=\u001b[39mtext_seed \u001b[38;5;241m+\u001b[39m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[38;5;241m=\u001b[39mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:220\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(samples)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(w, context) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FIT5217\\Lib\\site-packages\\nltk\\lm\\api.py:64\u001b[0m, in \u001b[0;36m_weighted_choice\u001b[1;34m(population, weights, random_generator)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like random.choice, but with weights.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mHeavily inspired by python 3.6 `random.choices`.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m population:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt choose from empty population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(population) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't choose from empty population"
     ]
    }
   ],
   "source": [
    "generate_sent(model_loaded, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4f13ed7359c87e397229104c1bcf18eb20603ad",
    "id": "9_mm_CPu1ASD"
   },
   "source": [
    "# Lets try some generating with Donald Trump data!!!\n",
    "\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/kingburrito666/better-donald-trump-tweets#Donald-Tweets!.csv\n",
    "\n",
    "\n",
    "In this part, I'll be munging that data as how I would be doing it at work. \n",
    "I've really no seen the data before but I hope this session would be helpful for you to see how to approach new datasets with the skills you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vs0vwDS35qvt",
    "outputId": "391f73f4-0cbb-4af9-d4d0-d6bc20450cdd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#replace the following path according to your Google Drive path\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "#replace the following path according to your Google Drive path\n",
    "%cd/gdrive/My Drive/Monash-FIT-S1-2022/week_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf5fea33f84e06c3cf613136ec33f80e774fc98",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "a2p43Oak1ASD",
    "outputId": "3201e51e-2ec9-4037-ed92-a05ebdf400a7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Donald-Tweets!.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a15d9d1a754ca357ae79e8390069b08b05320458",
    "id": "BOL2wzbO1ASD"
   },
   "outputs": [],
   "source": [
    "trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8724b8724cc52b685205047bd1fda649545a6a24",
    "id": "ZPVeSHho1ASD"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5010b607062f6f2dd0ccf011bb9004e1e3dcee7",
    "id": "8p3uPBQ71ASD"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8036f1ae5e0e1438d1d2981026766cdacd4479aa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lOgTjdSh1ASE",
    "outputId": "88a2378d-c8d3-4688-ff9a-b2dceeab653c"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=20, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "defe4e4899cb14300710eb0c7786a6f2e894455d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JU1teVBO1ASE",
    "outputId": "3ca46a16-82dc-4319-fa92-29834f654af2"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=10, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eba7934efa5a5e6b818e951c5daec9e9f621ca26",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6_Sxvfj_1ASE",
    "outputId": "3d5b1ece-e3ea-4d6d-d621-2024addb5528"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=50, random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec48d8d44b2f57b16249e12b845f43f52faadca",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2w6GjTLt1ASE",
    "outputId": "38590df2-9925-46f7-c760-209fde009e2c"
   },
   "outputs": [],
   "source": [
    "print(generate_sent(trump_model, num_words=100, random_seed=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2567d003d69c26233b915df03509b3fd014beed4",
    "id": "81mSlJ9y1ASE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "n-gram-language-model-with-nltk.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
