{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3712cd",
   "metadata": {},
   "source": [
    "**Neural Chef Assistance**  \n",
    "**Name: Darren Jer Shien Yee**  \n",
    "**Student ID: 31237223**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e2844",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: RNN without Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7c08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: tensorboardX in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (1.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (5.26.1)\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Requirements\n",
    "!pip3 install nltk\n",
    "!pip3 install tensorboardX\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d788a9e-58f9-4882-b4b6-5bf045a8c55c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Language Method imported from RNN code provided in tutorials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66b0337d-7e9f-474f-bbd1-83b5c9b887c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3c46-345e-4f6b-9fff-cff0800cd43c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Unicode and ASCII method to preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d787a58e-58ae-48f3-956c-fc7f98282e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "# Lowercase only (since numerical is crucial here as opposed to lab code)\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.replace('\\t', ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeb7d6-145e-4024-842e-ebc28427ad01",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Method to read data from provided CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec3f0e9-c616-41d4-92d6-c0e3f6cac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def readLangs(reverse=False):\n",
    "    # Read the CSV file\n",
    "    train_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/train.csv')\n",
    "    valid_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/dev.csv')\n",
    "    test_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/test.csv')\n",
    "    \n",
    "    train_df.fillna('', inplace=True)\n",
    "    valid_df.fillna('', inplace=True)\n",
    "    test_df.fillna('', inplace=True)\n",
    "    \n",
    "    train_ingredients = train_df['Ingredients']\n",
    "    train_recipes = train_df['Recipe']\n",
    "    valid_ingredients = valid_df['Ingredients']\n",
    "    valid_recipes = valid_df['Recipe']    \n",
    "    test_ingredients = test_df['Ingredients']\n",
    "    test_recipes = test_df['Recipe']\n",
    "    \n",
    "    train_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(train_ingredients, train_recipes)]\n",
    "    valid_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(valid_ingredients, valid_recipes)]\n",
    "    test_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(test_ingredients, test_recipes)]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('Recipe')\n",
    "        output_lang = Lang('Ingredients')\n",
    "    else:\n",
    "        input_lang = Lang('Ingredients')\n",
    "        output_lang = Lang('Recipe')\n",
    "    return input_lang, output_lang, train_pairs, valid_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372752f-bdba-432c-9fa7-5fd32712487a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Preprocess data and intialise language using the methods above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "add1f86a-2221-4fcd-a3ce-9181f68d197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 101340 sentence pairs\n",
      "Trimmed to 79894 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Ingredients 37056\n",
      "Recipe 35620\n",
      "['1 1/2 lb chicken breasts, boneless 1 c  scallion, coarsely chopped 2    eggs, well beaten 1/4 c  cornstarch 1 ts oriental sesame oil 1/2 ts salt 8 oz water chestnuts, drained chopped 16 sl firm white bread, crusts removed vegetable oil, for frying', 'cut chicken into chunks and put in a food processor . add scallion , eggs , cornstarch , sesame oil , and salt ; puree to a paste . transfer to a bowl and stir in water chestnuts . spread chicken paste over bread slices , cover , and refrigerate until ready to cook . in a large frying pan , heat 3/4 inch of oil over medium heat . add bread , chicken side down , and fry until golden brown , 1 to 2 minutes . drain on paper towels . cut into triangles and serve hot .']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 150\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "    \n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, train_pairs, valid_pairs, test_pairs = readLangs(reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(train_pairs))\n",
    "    train_pairs = filterPairs(train_pairs)\n",
    "    valid_pairs = filterPairs(valid_pairs)\n",
    "    test_pairs = filterPairs(test_pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(train_pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in train_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in valid_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in test_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, train_pairs, valid_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, valid_pairs, test_pairs = prepareData('ingredients', 'recipe', False)\n",
    "print(random.choice(train_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa22b6-5890-459f-a0de-eca4d17645d0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Encoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bad39fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170834eb-3f3b-466c-9fd2-76f05695b79a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Decoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2c193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90864fb4-1563-4b30-9e67-d3da22fe4ad2",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Processing methods for train iter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1296ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504668e0-5099-4670-b2b8-3688377eee99",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train method for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93e74aa1-b285-459a-ae49-fb99cfd3d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4d24171-4687-48c5-be89-08cf770315be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e73517-c02f-44d4-a0e5-e45f6c0a3be3",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Time method for train iter method** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30717ca1-4e2b-4a59-bd84-5eb1e68ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c1ccc-4c8a-4e72-b752-f0a12edf9cb0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train iter method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8c7ce3-911c-49f9-be91-524a542e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters,train_pairs,valid_pairs, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    print_valid_loss_total = 0  # Reset every print_every\n",
    "    plot_valid_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    valid_pairs = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        training_input_tensor = training_pair[0]\n",
    "        training_target_tensor = training_pair[1]\n",
    "\n",
    "        valid_pair = valid_pairs[iter - 1]\n",
    "        valid_input_tensor = valid_pair[0]\n",
    "        valid_target_tensor = valid_pair[1]\n",
    "\n",
    "        train_loss = train(training_input_tensor, training_target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        valid_loss = valid(valid_input_tensor, valid_target_tensor, encoder,\n",
    "             decoder, criterion)\n",
    "        print_valid_loss_total += valid_loss\n",
    "        plot_valid_loss_total += valid_loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_valid_loss_avg = print_valid_loss_total / print_every\n",
    "            print_valid_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f | Validation Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                                             iter, iter / n_iters * 100,\n",
    "                                                                             print_train_loss_avg, print_valid_loss_avg))\n",
    "        if iter % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_valid_loss_avg = plot_valid_loss_total / plot_every\n",
    "            plot_valid_losses.append(plot_valid_loss_avg)\n",
    "            plot_valid_loss_total = 0\n",
    "    return plot_train_losses,plot_valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96969eea-8ad7-4879-8fec-2e72756ae498",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Show Plot method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a09497-62ff-467c-8923-87e52b92fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "def showPlot(model_name,points1, points2, epochs, plot_every):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # This locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    x_range = np.array(np.arange(0, epochs-1, plot_every))\n",
    "    plt.plot(x_range,points1, label='Train')  # Plot points1 with label 'Train'\n",
    "    plt.plot(x_range,points2, label='Valid')  # Plot points2 with label 'Valid'\n",
    "    plt.legend()  # Add legend\n",
    "    plt.xlabel('Epochs')  # Label x-axis\n",
    "    plt.ylabel('Loss')  # Label y-axis\n",
    "    plt.title(model_name+' Training and Validation Loss')  # Set title for the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b9c14-0d88-4401-84f6-3641172bab96",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Evaluation methods to check performance of baseline 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05211516-e976-47d1-b914-449d0d3fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6fbea66-faee-4cb0-b11a-bb5312664000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words= evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdb0fe-aac9-48f4-b1fb-2e500d5b9c73",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Creating instance of baseline 1 and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "69d95a1d-094a-44e8-a874-246e81ff79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 54s (- 35m 13s) (1000 10%) Train Loss: 7.2178 | Validation Loss: 7.9970\n",
      "7m 53s (- 31m 33s) (2000 20%) Train Loss: 7.4112 | Validation Loss: 10.4096\n",
      "11m 56s (- 27m 52s) (3000 30%) Train Loss: 7.2786 | Validation Loss: 11.2426\n",
      "16m 4s (- 24m 6s) (4000 40%) Train Loss: 7.1564 | Validation Loss: 11.4670\n",
      "20m 9s (- 20m 9s) (5000 50%) Train Loss: 6.9858 | Validation Loss: 11.8815\n",
      "23m 55s (- 15m 56s) (6000 60%) Train Loss: 7.0102 | Validation Loss: 12.1273\n",
      "27m 40s (- 11m 51s) (7000 70%) Train Loss: 6.8358 | Validation Loss: 12.3677\n",
      "31m 32s (- 7m 53s) (8000 80%) Train Loss: 7.0890 | Validation Loss: 12.3633\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m baseline1_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m baseline1_decoder \u001b[38;5;241m=\u001b[39m DecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m baseline1_plot_train_losses,baseline1_plot_valid_losses \u001b[38;5;241m=\u001b[39m trainIters(baseline1_encoder, baseline1_decoder, n_iter ,train_pairs,valid_pairs, print_every \u001b[38;5;241m=\u001b[39m print_every,plot_every\u001b[38;5;241m=\u001b[39mplot_every)\n",
      "Cell \u001b[1;32mIn[91], line 32\u001b[0m, in \u001b[0;36mtrainIters\u001b[1;34m(encoder, decoder, n_iters, train_pairs, valid_pairs, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     29\u001b[0m print_train_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[0;32m     30\u001b[0m plot_train_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m---> 32\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m valid(valid_input_tensor, valid_target_tensor, encoder,\n\u001b[0;32m     33\u001b[0m      decoder, criterion)\n\u001b[0;32m     34\u001b[0m print_valid_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_loss\n\u001b[0;32m     35\u001b[0m plot_valid_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_loss\n",
      "Cell \u001b[1;32mIn[89], line 19\u001b[0m, in \u001b[0;36mvalid\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, criterion, max_length)\u001b[0m\n\u001b[0;32m     17\u001b[0m decoder_hidden \u001b[38;5;241m=\u001b[39m encoder_hidden\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[1;32m---> 19\u001b[0m     decoder_output, decoder_hidden \u001b[38;5;241m=\u001b[39m decoder(\n\u001b[0;32m     20\u001b[0m         decoder_input, decoder_hidden)\n\u001b[0;32m     21\u001b[0m     topv, topi \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m topi\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# detach from history as input\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[86], line 14\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(output)\n\u001b[1;32m---> 14\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(output, hidden)\n\u001b[0;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(output[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1133\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1134\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1137\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_iter = 10000\n",
    "plot_every = 100\n",
    "print_every = 1000\n",
    "baseline1_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "baseline1_decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "baseline1_plot_train_losses,baseline1_plot_valid_losses = trainIters(baseline1_encoder, baseline1_decoder, n_iter ,train_pairs,valid_pairs, print_every = print_every,plot_every=plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fdb58-f4b2-4987-b631-98a20ae95de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPlot('Baseline 1',baseline1_plot_train_losses,baseline1_plot_valid_losses,n_iter,plot_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c35846-ac86-452b-9b8c-54f465a1f9ba",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814880f7-ef67-460c-980c-09b6745faa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        #self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        attn_weights = F.softmax(torch.bmm(hidden, encoder_outputs.T.unsqueeze(0)),dim=-1)\n",
    "        attn_output = torch.bmm(attn_weights, encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "\n",
    "        # attn_weights = F.softmax(\n",
    "        #     self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "        #                          encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        # output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # output = F.relu(output)\n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f95b7f-3c49-4a84-ade1-3242c219489f",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7f5d38e-94a3-47d4-b987-b9769d95ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "\n",
    "def train_attn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fabffd3-ee3e-4ffd-b360-9b7b1e0fcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "def valid_attn(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486009e-c90f-401d-ab3e-0423519d0b10",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train iter method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4ac4ebb-e010-4984-97d3-5b223b228062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters_attn(encoder, decoder, n_iters,print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    print_valid_loss_total = 0  # Reset every print_every\n",
    "    plot_valid_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    validation_pairs = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        training_input_tensor = training_pair[0]\n",
    "        training_target_tensor = training_pair[1]\n",
    "\n",
    "        valid_pair = validation_pairs[iter - 1]\n",
    "        valid_input_tensor = valid_pair[0]\n",
    "        valid_target_tensor = valid_pair[1]\n",
    "\n",
    "        train_loss = train_attn(training_input_tensor, training_target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        valid_loss = valid_attn(valid_input_tensor, valid_target_tensor, encoder,\n",
    "             decoder, criterion)\n",
    "        print_valid_loss_total += valid_loss\n",
    "        plot_valid_loss_total += valid_loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_valid_loss_avg = print_valid_loss_total / print_every\n",
    "            print_valid_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f | Validation Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                                             iter, iter / n_iters * 100,\n",
    "                                                                             print_train_loss_avg, print_valid_loss_avg))\n",
    "        if iter % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_valid_loss_avg = plot_valid_loss_total / plot_every\n",
    "            plot_valid_losses.append(plot_valid_loss_avg)\n",
    "            plot_valid_loss_total = 0\n",
    "    return plot_train_losses,plot_valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942fc9b-e5f6-41cf-af02-116ca14b1874",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Initialising instance of RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "735b4dee-5b75-4325-ae4a-267a1fb93592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 30s (- 4m 34s) (100 10%) Train Loss: 6.8305 | Validation Loss: 4.5626\n",
      "0m 55s (- 3m 43s) (200 20%) Train Loss: 5.6071 | Validation Loss: 3.5437\n",
      "1m 24s (- 3m 16s) (300 30%) Train Loss: 5.1862 | Validation Loss: 5.2328\n",
      "1m 51s (- 2m 47s) (400 40%) Train Loss: 5.2521 | Validation Loss: 3.9226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m baseline2_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m attn_decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m trainIters_attn(baseline2_encoder, attn_decoder, \u001b[38;5;241m1000\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 32\u001b[0m, in \u001b[0;36mtrainIters_attn\u001b[1;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     29\u001b[0m print_train_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[0;32m     30\u001b[0m plot_train_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m---> 32\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m valid_attn(valid_input_tensor, valid_target_tensor, encoder,\n\u001b[0;32m     33\u001b[0m      decoder, criterion)\n\u001b[0;32m     34\u001b[0m print_valid_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_loss\n\u001b[0;32m     35\u001b[0m plot_valid_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_loss\n",
      "Cell \u001b[1;32mIn[46], line 13\u001b[0m, in \u001b[0;36mvalid_attn\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, criterion, max_length)\u001b[0m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_length):\n\u001b[1;32m---> 13\u001b[0m     encoder_output, encoder_hidden \u001b[38;5;241m=\u001b[39m encoder(\n\u001b[0;32m     14\u001b[0m         input_tensor[ei], encoder_hidden)\n\u001b[0;32m     15\u001b[0m     encoder_outputs[ei] \u001b[38;5;241m=\u001b[39m encoder_output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[SOS_token]], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m, in \u001b[0;36mEncoderRNN.forward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[1;32m---> 10\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m embedded\n\u001b[0;32m     12\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(output, hidden)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_iter = 10000\n",
    "plot_every = 100\n",
    "print_every = 100\n",
    "baseline2_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "trainIters_attn(baseline2_encoder, attn_decoder, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23e78a-7de0-4a1e-8f37-23c8d1d05668",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Evaluation method for RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e16ce082-cf7e-471b-9488-615f4ddc8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attn(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d6af3899-43bc-409e-8303-d139e8d108c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly_attn(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attention= evaluate_attn(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "48da6244-ed20-456d-bcfc-f3494ab6e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1 c  unsifted all-purpose flour 1/2 ts baking powder 1/2 ts salt 1/2 ts baking soda 1/2 c  butter or margarine 1/2 c  sugar 1/2 c  brown sugar 1    egg 1 ts vanilla 1/2 c  quick cooking oats 1 c  chopped peeled apple 2 c  coconut\n",
      "= mix flour with baking powder , salt & soda . beat butter , gradually beat in sugars until light & fluffy . blend in egg & vanilla . blend in flour mixture , add oats , apple & 11/3 cups coconut . drop by teaspoonfuls onto ungreaed baking sheets . sprinkle with remaining coconut . bake at 375 degrees about 10 min .\n",
      "< sift together flour and baking powder . add salt and pepper to taste . add sugar and cook for 5 minutes . <EOS>\n",
      "\n",
      "> 1/3 lb ground beef per person\n",
      "= brown the ground beef thoroughly in the frying pan and drain off excess fat . add whatever you want like onion , green pepper , celery , can of soup cheese , a small can of spaghetti or beans or macaroni or corn or tomatoes and seasoning .\n",
      "< preheat oven to 350 degrees . grease flour and baking powder and add salt and pepper to taste . cook for 5 minutes . add tomato sauce , stirring to sauce . cook for 5 minutes . <EOS>\n",
      "\n",
      "> 3 t  margarine or butter 1 lb round steak, cut into thin strips 1/2 c  chopped onion 4 oz sliced mushrooms, drained 1/4 t  dry mustard 1/2 t  salt 1/4 t  pepper 8 oz cream cheese, cubed 2/3 c  milk hot parsleyed noodles\n",
      "= place 3 tablespoons butter or margarine in a deep , 2-quart , heat-resistant , non-metallic casserole . melt in microwave oven 30 seconds . add steak strips to butter and heat , uncovered , in microwave oven 10 minutes or until meat is browned . add onion , mushrooms and seasoning to meat . heat , uncovered , in microwave oven 4 minutes . add cubed cream cheese and milk to meat mixture and heat , uncovered , in microwave oven 4 minutes or until cheese melts . stir occasionally . serve over hot parsleyed noodles .\n",
      "< cook pasta until tender . drain well , reserving liquid in a bowl . add to pan . cook for 5 minutes . add water and cook for 5 minutes . stir in the beans and stir to 2 minutes . remove to cool . cook for 5 minutes . stir in the pasta and stir to the sauce . season with salt and pepper . <EOS>\n",
      "\n",
      "> 8 lb pork tenderloin (boneless) 1 1/3 c  soy sauce 2/3 c  oriental toasted sesame oil 4 ea minced garlic cloves (large) 1 t  ground ginger (fresh) 1 t  msg (if desired) 19 oz bottled bar-b-q sauce\n",
      "= place pork into marinade and place in refrigerator for 6 to 8 hours -lrb- over night if you prefer -rrb- . remove pork from marinade and place on covered grill . add wet wood to grill firepan to insure adequate supply of smoke . combine bar-b-q sauce , 1/3 c sesame oil , 1/3 c soy sauce and 1 minced garlic clove in a bowl and mix well . serve over sliced pork tenderloins .\n",
      "< place in a saucepan . cook for 5 minutes . drain & onions in a warm . saute onion & garlic in oil until softened . add onion & cook for another 5 minutes . add flour and cook for 5 minutes . drain & mash with water and cook for 5 minutes . add water and cook for 5 minutes . add flour and water to cook for 2 minutes . drain and stir well . cook for 5 minutes . add flour and cook for 2 minutes . stir in rice and cook for 5 minutes . drain and stir into water and cook for 5 minutes . add water and cook for 5 minutes . serve hot . <EOS>\n",
      "\n",
      "> 1/2 c  chopped walnuts 1/2 c  butter or margarine 3 tb madeira wine\n",
      "= in skillet , saute walnuts in 2 tablespoons of the butter until golden and fragrant . reduce heat and add remaining 6 tablespoons butter ; stir until melted . stir in madeira . serve warm . serve with grilled trout\n",
      "< melt butter in a saucepan . add sugar and cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . remove to a boil . add sugar and cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . add sugar and cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . stir in the beans , stirring occasionally , until the liquid has evaporated in the refrigerator . <EOS>\n",
      "\n",
      "> 4 c  flour 4 c  crakermeal -- or ground (inexpensive crackers 4 tb salt 2 tb sugar 2 ts garlic powder 2 ts onion powder 3 tb paprika 1/4 c  vegetable oil\n",
      "= mix and store indefinitely in the refrigerator in a covered container .\n",
      "< blend all ingredients in a saucepan . cook over medium heat until smooth . remove to a boil . reduce heat , cover and simmer for 30 minutes . remove to cool . remove to pan . add to pan , cover and simmer for 30 minutes . remove to cool . cook and stir until mixture is smooth . cook over medium heat until mixture is heated through . serve hot . <EOS>\n",
      "\n",
      "> 4 md ears of corn 2 tb butter or margarine 2 tb fresh lime juice 1/2 ts pepper (optional)\n",
      "= using a cleaver or large knove , cut corn into 1-in lengths . in a large pot , bring water to boil over high heat . add corn wheels ; cover and boil 2 minutes . melt butter , mix with lime juice . transfer corn to serving platter and drizzle with lime-butter . sprinkle with pepper , if desired .\n",
      "< cook pasta and drain in a large bowl , add to sauce and cook for 5 minutes . drain . meanwhile , heat sauce in a saucepan . add flour and cook for 5 minutes . add water and cook for 5 minutes . add water and cook for 5 minutes . remove to pan . add to pan , cover and simmer for 30 minutes . remove to pan . add to pan , stirring to prevent to prevent sticking . <EOS>\n",
      "\n",
      "> 8 oz cornbread stuffing mix 1 c  water 1/3 c  butter, melted 1/2 c  sweetened dried cranberries 8 3/4 oz whole kernel corn, drained 12    thick slices deli turkey  breast (about 2 pounds) 12 oz turkey gravy\n",
      "= preheat the oven to 375 degrees f . in a medium-sized bowl , combine the stuffing mix , water , butter , cranberries , and corn ; mix well . place 1/4 cup of the stuffing mixture at the smaller end of each slice of turkey and roll up crepe-style . make sure to ask the deli to slice the turkey thickly so that there are about 6 slices to the pound .\n",
      "< cook pasta until al dente . drain & set aside . saute onion & garlic in butter until softened . add onion & cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . add flour and cook for 5 minutes . add salt and pepper to taste . cook pasta and simmer for 30 minutes . <EOS>\n",
      "\n",
      "> 12 ea canning tomatoes 1 ea sm onion 1 c  chopped green chili 1 ts salt 1 ts cumino 1 ts oregano\n",
      "= dip tomatoes in boiling water , peel and chop . chop small onions . put tomato , onion and chili in 6 quart pan . add seasonings and simmer for 11/2 hours . put in canning jars . bath in hot water bath for 30 min .\n",
      "< cook pasta according to package directions . drain and set aside . drain in the fat . season with salt . <EOS>\n",
      "\n",
      "> 1 1/2 lb broccoli 2 tb butter or margarine 1/2 c  chopped onion 1 md clove garlic; minced 1 cn campbell's soup  1/2 ts dried basil leaf; crushed 1 c  monterey jack cheese  1/2 c  parmesan cheese; grated 1 c  sour cream 5 c  cooked wide egg noodles\n",
      "= cooking cut broccoli into bite-size pieces . in covered 4-qt saucepan over medium heat , in 1 '' boiling water , cook broccoli 6 mins or until tender . drain in colander . in same saucepan over medium heat , in hot butter , cook onion and garlic until onion is tender , stirring occasionally . stir in soup and basil ; mix well . add cheeses , stirring until melted . stir in sour cream , broccoli and cooked noodles . pour into 2-qt casserole . cover ; bake at 350 f for 30 mins or until bubbly .\n",
      "< melt butter in a saucepan . add onion & cook for another 5 minutes . add eggs . cook for 5 minutes . add water and cook for 5 minutes . add flour and cook for 5 minutes . stir in flour and water to make a soft dough . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly_attn(baseline2_encoder, baseline2_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93235ea-8dcd-462d-945e-712849d4c172",
   "metadata": {},
   "source": [
    "**Extension 1: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6c726-d186-4327-b401-9b31865fdc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4fac582-4b7a-4825-a9ec-54624622a757",
   "metadata": {},
   "source": [
    "**Extension 2: Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20845898-86b7-4d01-af5e-aca296af2a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
