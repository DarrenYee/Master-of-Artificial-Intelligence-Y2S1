{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3712cd",
   "metadata": {},
   "source": [
    "**Neural Chef Assistance**  \n",
    "**Name: Darren Jer Shien Yee**  \n",
    "**Student ID: 31237223**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e2844",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: RNN without Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7c08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: tensorboardX in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (1.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\manut\\anaconda3\\envs\\fit5217\\lib\\site-packages (from tensorboardX) (5.26.1)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Requirements\n",
    "!pip3 install nltk\n",
    "!pip3 install tensorboardX\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d788a9e-58f9-4882-b4b6-5bf045a8c55c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Language Method imported from RNN code provided in tutorials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b0337d-7e9f-474f-bbd1-83b5c9b887c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "        self.unique_words = []\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.unique_words.append(word)\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3c46-345e-4f6b-9fff-cff0800cd43c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Unicode and ASCII method to preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d787a58e-58ae-48f3-956c-fc7f98282e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "# Lowercase only (since numerical is crucial here as opposed to lab code)\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.replace('\\t', ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeb7d6-145e-4024-842e-ebc28427ad01",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Method to read data from provided CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec3f0e9-c616-41d4-92d6-c0e3f6cac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def readLangs(reverse=False):\n",
    "    # Read the CSV file\n",
    "    train_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/train.csv')\n",
    "    valid_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/dev.csv')\n",
    "    test_df = pd.read_csv('Cooking_Dataset/Cooking_Dataset/test.csv')\n",
    "    \n",
    "    train_df.fillna('', inplace=True)\n",
    "    valid_df.fillna('', inplace=True)\n",
    "    test_df.fillna('', inplace=True)\n",
    "    \n",
    "    train_ingredients = train_df['Ingredients']\n",
    "    train_recipes = train_df['Recipe']\n",
    "    valid_ingredients = valid_df['Ingredients']\n",
    "    valid_recipes = valid_df['Recipe']    \n",
    "    test_ingredients = test_df['Ingredients']\n",
    "    test_recipes = test_df['Recipe']\n",
    "    \n",
    "    train_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(train_ingredients, train_recipes)]\n",
    "    valid_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(valid_ingredients, valid_recipes)]\n",
    "    test_pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(test_ingredients, test_recipes)]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('Recipe')\n",
    "        output_lang = Lang('Ingredients')\n",
    "    else:\n",
    "        input_lang = Lang('Ingredients')\n",
    "        output_lang = Lang('Recipe')\n",
    "    return input_lang, output_lang, train_pairs, valid_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372752f-bdba-432c-9fa7-5fd32712487a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Preprocess data and intialise language using the methods above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add1f86a-2221-4fcd-a3ce-9181f68d197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 101340 sentence pairs\n",
      "Trimmed to 79894 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Ingredients 37056\n",
      "Recipe 35620\n",
      "['2 lb smoked halibut 1 c  butter 1    garlic clove 1 pn onion salt 1/4 ts marjoram 1/8 ts thyme 1 pn ground oregano', 'melt butter in saucepan over low heat . add whole garlic , seasonings and herbs . remove from heat and let steep for an hour . place halibut in baking dish . discard garlic and pour herb butter over fish . warm up in microwave or oven at 350 degrees f. for 15 minutes . andrea , cordova , alaska']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 150\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "    \n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, train_pairs, valid_pairs, test_pairs = readLangs(reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(train_pairs))\n",
    "    train_pairs = filterPairs(train_pairs)\n",
    "    valid_pairs = filterPairs(valid_pairs)\n",
    "    test_pairs = filterPairs(test_pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(train_pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in train_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in valid_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in test_pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, train_pairs, valid_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, valid_pairs, test_pairs = prepareData('ingredients', 'recipe', False)\n",
    "print(random.choice(train_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa22b6-5890-459f-a0de-eca4d17645d0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Encoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bad39fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170834eb-3f3b-466c-9fd2-76f05695b79a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Decoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90864fb4-1563-4b30-9e67-d3da22fe4ad2",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Processing methods for train iter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1296ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504668e0-5099-4670-b2b8-3688377eee99",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train method for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93e74aa1-b285-459a-ae49-fb99cfd3d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4d24171-4687-48c5-be89-08cf770315be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e73517-c02f-44d4-a0e5-e45f6c0a3be3",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Time method for train iter method** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30717ca1-4e2b-4a59-bd84-5eb1e68ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c1ccc-4c8a-4e72-b752-f0a12edf9cb0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train iter method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c8c7ce3-911c-49f9-be91-524a542e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters,train_pairs,valid_pairs, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    print_valid_loss_total = 0  # Reset every print_every\n",
    "    plot_valid_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    valid_pairs = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        training_input_tensor = training_pair[0]\n",
    "        training_target_tensor = training_pair[1]\n",
    "\n",
    "        valid_pair = valid_pairs[iter - 1]\n",
    "        valid_input_tensor = valid_pair[0]\n",
    "        valid_target_tensor = valid_pair[1]\n",
    "\n",
    "        train_loss = train(training_input_tensor, training_target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        valid_loss = valid(valid_input_tensor, valid_target_tensor, encoder,\n",
    "             decoder, criterion)\n",
    "        print_valid_loss_total += valid_loss\n",
    "        plot_valid_loss_total += valid_loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_valid_loss_avg = print_valid_loss_total / print_every\n",
    "            print_valid_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f | Validation Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                                             iter, iter / n_iters * 100,\n",
    "                                                                             print_train_loss_avg, print_valid_loss_avg))\n",
    "        if iter % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_valid_loss_avg = plot_valid_loss_total / plot_every\n",
    "            plot_valid_losses.append(plot_valid_loss_avg)\n",
    "            plot_valid_loss_total = 0\n",
    "    return plot_train_losses,plot_valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96969eea-8ad7-4879-8fec-2e72756ae498",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Show Plot method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a09497-62ff-467c-8923-87e52b92fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "def showPlot(model_name,points1, points2, epochs, plot_every):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # This locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    x_range = np.array(np.arange(0, epochs-1, plot_every))\n",
    "    plt.plot(x_range,points1, label='Train')  # Plot points1 with label 'Train'\n",
    "    plt.plot(x_range,points2, label='Valid')  # Plot points2 with label 'Valid'\n",
    "    plt.legend()  # Add legend\n",
    "    plt.xlabel('Epochs')  # Label x-axis\n",
    "    plt.ylabel('Loss')  # Label y-axis\n",
    "    plt.title(model_name+' Training and Validation Loss')  # Set title for the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b9c14-0d88-4401-84f6-3641172bab96",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Evaluation methods to check performance of baseline 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05211516-e976-47d1-b914-449d0d3fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6fbea66-faee-4cb0-b11a-bb5312664000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words= evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdb0fe-aac9-48f4-b1fb-2e500d5b9c73",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Creating instance of baseline 1 and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69d95a1d-094a-44e8-a874-246e81ff79d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plot_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 5\u001b[0m baseline1_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m baseline1_decoder \u001b[38;5;241m=\u001b[39m DecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m baseline1_plot_train_losses,baseline1_plot_valid_losses \u001b[38;5;241m=\u001b[39m trainIters(baseline1_encoder, baseline1_decoder, n_iter ,train_pairs,valid_pairs, print_every \u001b[38;5;241m=\u001b[39m print_every,plot_every\u001b[38;5;241m=\u001b[39mplot_every)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_iter = 10000\n",
    "plot_every = 100\n",
    "print_every = 1000\n",
    "baseline1_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "baseline1_decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "baseline1_plot_train_losses,baseline1_plot_valid_losses = trainIters(baseline1_encoder, baseline1_decoder, n_iter ,train_pairs,valid_pairs, print_every = print_every,plot_every=plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fdb58-f4b2-4987-b631-98a20ae95de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPlot('Baseline 1',baseline1_plot_train_losses,baseline1_plot_valid_losses,n_iter,plot_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c35846-ac86-452b-9b8c-54f465a1f9ba",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814880f7-ef67-460c-980c-09b6745faa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        #self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        attn_weights = F.softmax(torch.bmm(hidden, encoder_outputs.T.unsqueeze(0)),dim=-1)\n",
    "        attn_output = torch.bmm(attn_weights, encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "\n",
    "        # attn_weights = F.softmax(\n",
    "        #     self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "        #                          encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        # output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # output = F.relu(output)\n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f95b7f-3c49-4a84-ade1-3242c219489f",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7f5d38e-94a3-47d4-b987-b9769d95ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "\n",
    "def train_attn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fabffd3-ee3e-4ffd-b360-9b7b1e0fcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "def valid_attn(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486009e-c90f-401d-ab3e-0423519d0b10",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train iter method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4ac4ebb-e010-4984-97d3-5b223b228062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters_attn(encoder, decoder, n_iters,print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    print_train_loss_total = 0  # Reset every print_every\n",
    "    plot_train_loss_total = 0  # Reset every plot_every\n",
    "    print_valid_loss_total = 0  # Reset every print_every\n",
    "    plot_valid_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    validation_pairs = [tensorsFromPair(random.choice(valid_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        training_input_tensor = training_pair[0]\n",
    "        training_target_tensor = training_pair[1]\n",
    "\n",
    "        valid_pair = validation_pairs[iter - 1]\n",
    "        valid_input_tensor = valid_pair[0]\n",
    "        valid_target_tensor = valid_pair[1]\n",
    "\n",
    "        train_loss = train_attn(training_input_tensor, training_target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_train_loss_total += train_loss\n",
    "        plot_train_loss_total += train_loss\n",
    "        \n",
    "        valid_loss = valid_attn(valid_input_tensor, valid_target_tensor, encoder,\n",
    "             decoder, criterion)\n",
    "        print_valid_loss_total += valid_loss\n",
    "        plot_valid_loss_total += valid_loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_train_loss_avg = print_train_loss_total / print_every\n",
    "            print_train_loss_total = 0\n",
    "            print_valid_loss_avg = print_valid_loss_total / print_every\n",
    "            print_valid_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f | Validation Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                                             iter, iter / n_iters * 100,\n",
    "                                                                             print_train_loss_avg, print_valid_loss_avg))\n",
    "        if iter % plot_every == 0:\n",
    "            plot_train_loss_avg = plot_train_loss_total / plot_every\n",
    "            plot_train_losses.append(plot_train_loss_avg)\n",
    "            plot_train_loss_total = 0\n",
    "            \n",
    "            plot_valid_loss_avg = plot_valid_loss_total / plot_every\n",
    "            plot_valid_losses.append(plot_valid_loss_avg)\n",
    "            plot_valid_loss_total = 0\n",
    "    return plot_train_losses,plot_valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942fc9b-e5f6-41cf-af02-116ca14b1874",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Initialising instance of RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "735b4dee-5b75-4325-ae4a-267a1fb93592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 59s (- 44m 57s) (1000 10%) Train Loss: 5.0060 | Validation Loss: 5.7207\n",
      "9m 55s (- 39m 42s) (2000 20%) Train Loss: 4.1286 | Validation Loss: 7.2443\n",
      "14m 45s (- 34m 26s) (3000 30%) Train Loss: 3.8955 | Validation Loss: 6.5614\n",
      "19m 38s (- 29m 28s) (4000 40%) Train Loss: 3.7622 | Validation Loss: 6.5764\n",
      "24m 46s (- 24m 46s) (5000 50%) Train Loss: 3.7299 | Validation Loss: 6.9598\n",
      "29m 30s (- 19m 40s) (6000 60%) Train Loss: 3.6491 | Validation Loss: 6.4495\n",
      "34m 13s (- 14m 40s) (7000 70%) Train Loss: 3.6353 | Validation Loss: 6.7779\n",
      "38m 55s (- 9m 43s) (8000 80%) Train Loss: 3.6191 | Validation Loss: 6.1473\n",
      "43m 49s (- 4m 52s) (9000 90%) Train Loss: 3.4564 | Validation Loss: 6.6598\n",
      "48m 41s (- 0m 0s) (10000 100%) Train Loss: 3.5469 | Validation Loss: 6.7105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([6.809228181963516,\n",
       "  5.640570060866022,\n",
       "  5.278540322663254,\n",
       "  4.990064319763482,\n",
       "  4.8706591112805935,\n",
       "  4.75482755430752,\n",
       "  4.564559758050934,\n",
       "  4.311348940873211,\n",
       "  4.48589132458738,\n",
       "  4.354048506982548,\n",
       "  4.3049282500059824,\n",
       "  4.370688218039263,\n",
       "  4.201784072876013,\n",
       "  3.9883165850411473,\n",
       "  4.13628815160465,\n",
       "  4.146663711976898,\n",
       "  4.08440164926603,\n",
       "  4.36289762875585,\n",
       "  3.8208266411115823,\n",
       "  3.8695859251754148,\n",
       "  3.9420229549950925,\n",
       "  3.9496978686822457,\n",
       "  3.8182430414985884,\n",
       "  3.992541015913137,\n",
       "  3.8970326910528614,\n",
       "  3.885791905455929,\n",
       "  3.9258169846639235,\n",
       "  3.9138098370314895,\n",
       "  3.8073380145087263,\n",
       "  3.822285724344025,\n",
       "  3.8025342096082255,\n",
       "  3.7236479054172054,\n",
       "  3.7693651632223277,\n",
       "  3.800639717502825,\n",
       "  3.7262704433293834,\n",
       "  3.7947960392653926,\n",
       "  3.6869427816958673,\n",
       "  3.646665841541994,\n",
       "  3.838052037874645,\n",
       "  3.8333901461361632,\n",
       "  3.7032628959362968,\n",
       "  3.664244943960061,\n",
       "  3.8412705712376134,\n",
       "  3.6668013015625593,\n",
       "  3.6943594268916895,\n",
       "  3.9228384712020348,\n",
       "  3.719400228833754,\n",
       "  3.5469441474531145,\n",
       "  3.7250328425854007,\n",
       "  3.814589066268906,\n",
       "  3.8313263465845573,\n",
       "  3.6608077606105867,\n",
       "  3.475389437713555,\n",
       "  3.586448868167354,\n",
       "  3.6439425987645935,\n",
       "  3.855199829335246,\n",
       "  3.681942771153745,\n",
       "  3.4920454122350697,\n",
       "  3.633409692446614,\n",
       "  3.630978708626486,\n",
       "  3.8261706378624543,\n",
       "  3.6154923537312254,\n",
       "  3.6801366227815255,\n",
       "  3.5304847515305324,\n",
       "  3.8751601500911077,\n",
       "  3.463451979243658,\n",
       "  3.6127922413315785,\n",
       "  3.4312330210710735,\n",
       "  3.5990402351330744,\n",
       "  3.7187510866959896,\n",
       "  3.6257627022219143,\n",
       "  3.6143068814099935,\n",
       "  3.6246657228946697,\n",
       "  3.4455358642834244,\n",
       "  3.6048147867356657,\n",
       "  3.6210877270009973,\n",
       "  3.5275811694228763,\n",
       "  3.7418354913659124,\n",
       "  3.6790994789207163,\n",
       "  3.7065677925891776,\n",
       "  3.416258714733171,\n",
       "  3.615872991262783,\n",
       "  3.414480167189877,\n",
       "  3.6523033465091626,\n",
       "  3.3537352083075898,\n",
       "  3.5672291844949164,\n",
       "  3.359208966669302,\n",
       "  3.4256229358976085,\n",
       "  3.440316851581235,\n",
       "  3.3185845595606582,\n",
       "  3.7558236296346603,\n",
       "  3.5371536698866963,\n",
       "  3.6208592911772097,\n",
       "  3.5749734344388706,\n",
       "  3.405532351493938,\n",
       "  3.495489053411441,\n",
       "  3.460029333169332,\n",
       "  3.5720398118853347,\n",
       "  3.6391837811524885,\n",
       "  3.4078053035001226],\n",
       " [4.522896037444201,\n",
       "  3.687306730026815,\n",
       "  4.366389391304494,\n",
       "  4.254109405641951,\n",
       "  4.631757665358774,\n",
       "  6.001540435207895,\n",
       "  7.938321526730106,\n",
       "  7.947445866961066,\n",
       "  6.761400209970834,\n",
       "  7.096029591217657,\n",
       "  7.578012718529999,\n",
       "  7.730677398212371,\n",
       "  7.747306667086912,\n",
       "  7.6702069969839295,\n",
       "  7.212917369709342,\n",
       "  6.64787965089297,\n",
       "  6.958306303008277,\n",
       "  6.442585071975853,\n",
       "  6.973861776836405,\n",
       "  7.481459549064931,\n",
       "  7.158600522906177,\n",
       "  5.567385021593068,\n",
       "  6.701998247055143,\n",
       "  6.462502056058558,\n",
       "  6.162031648127941,\n",
       "  6.097622969750452,\n",
       "  6.1959994497148925,\n",
       "  6.952247968155968,\n",
       "  7.229018481725621,\n",
       "  7.086193495859434,\n",
       "  5.707821001528235,\n",
       "  5.741250118562139,\n",
       "  6.513160336508654,\n",
       "  7.157381996123692,\n",
       "  6.950109305715767,\n",
       "  6.585440218400977,\n",
       "  6.683274574598682,\n",
       "  6.739088223700739,\n",
       "  6.625882253010548,\n",
       "  7.060597900607352,\n",
       "  7.681426796876081,\n",
       "  7.0778273449169555,\n",
       "  5.857765141350278,\n",
       "  7.219536322396497,\n",
       "  6.3970886882040086,\n",
       "  7.821902741276766,\n",
       "  6.856638831848184,\n",
       "  6.771714023232476,\n",
       "  7.177362901105718,\n",
       "  6.736773572843336,\n",
       "  6.647986908231602,\n",
       "  6.003010748872838,\n",
       "  5.478831281748237,\n",
       "  6.6704653171980555,\n",
       "  6.498759561607892,\n",
       "  6.468119965528947,\n",
       "  6.960818764265907,\n",
       "  6.489006429123314,\n",
       "  6.593254182230034,\n",
       "  6.685240316319738,\n",
       "  6.1193903301329104,\n",
       "  7.033928418248944,\n",
       "  6.804225069196915,\n",
       "  6.507576532053388,\n",
       "  6.332619242792519,\n",
       "  7.040015228037464,\n",
       "  7.895691864204398,\n",
       "  6.251836629354277,\n",
       "  6.760409398806556,\n",
       "  7.0331439130034905,\n",
       "  6.876692445253898,\n",
       "  6.599796110093238,\n",
       "  6.208305566987168,\n",
       "  5.267819833949302,\n",
       "  5.955398371066378,\n",
       "  5.634586222013188,\n",
       "  6.552458418055035,\n",
       "  5.724642755067667,\n",
       "  5.908280044743304,\n",
       "  6.7448470082033625,\n",
       "  6.169300511435056,\n",
       "  6.218602680826546,\n",
       "  6.210210709552921,\n",
       "  5.889243868296634,\n",
       "  6.265950436263098,\n",
       "  7.500848509158399,\n",
       "  7.181942201595637,\n",
       "  7.1356002502359175,\n",
       "  7.2527486652284825,\n",
       "  6.773461342996102,\n",
       "  6.420963260784754,\n",
       "  6.627734937228948,\n",
       "  6.063838057708953,\n",
       "  7.037334353492554,\n",
       "  6.9862491589959,\n",
       "  6.9177784336747745,\n",
       "  6.513944481365371,\n",
       "  6.833749673291134,\n",
       "  6.891150804291613,\n",
       "  6.811925335711484])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_iter = 10000\n",
    "plot_every = 100\n",
    "print_every = 100\n",
    "baseline2_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "trainIters_attn(baseline2_encoder, attn_decoder, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23e78a-7de0-4a1e-8f37-23c8d1d05668",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Evaluation method for RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e16ce082-cf7e-471b-9488-615f4ddc8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attn(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6af3899-43bc-409e-8303-d139e8d108c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly_attn(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(train_pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attention= evaluate_attn(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48da6244-ed20-456d-bcfc-f3494ab6e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 4 fl coconut cream 4 fl chicken stock 2 tb vegetable oil 2    kaffir lime leaves,roughly 1    garlic clove finely chopped -chopped 1 tb red curry paste 4 oz bamboo shoots, cut into 2 tb fish sauce -slivers 1 ts sugar 20    fresh holy basil leaves 6 oz boneless chicken breasts\n",
      "= in a wok or frying pan heat the oil and fry the garlic until golden brown . add the curry paste and stir well . pour in the warmed coconut cream and stir until it begins to reduce and thicken . add the fish sauce and sugar and stir . add the chicken and cook , stirring constantly , until the meat is opaque . add the stock , stir and cook for one to two min utes or until the chicken is cooked through . stir in the lime leaves , then add the bamboo shoots and basil leaves . stir , cook gently for a final minute and turn into a serving dish .\n",
      "< in a large skillet , heat the oil and fry the chicken pieces until crisp . add the cream and cook until the onions are soft , about 10 minutes . add the stock , and cook until the onions are soft . add the chopped chiles and cook until the onions are soft . add the cream and mix well . add the chopped tomatoes and cook for a few seconds . add the cream and mix well . add the bouquet garni and toss . serve with tortilla chips . <EOS>\n",
      "\n",
      "> 1 ea brie round 1 ea pasrty dough 1 ea egg 2 t  milk\n",
      "= completty enclose cheese in pastry . make egg wash of egg and milk . put brie on baking sheet with seam side of dough down . brush with egg wash . cut garnish of leaves , flowers , etc. , from scraps of pastry and decorate top of brie . bake at 350f about 30 minutes or until golden brown .\n",
      "< in a small saucepan , combine the flour , salt , baking powder , and salt . add the milk and stir until melted . add the milk and cook until the onions are soft . add the milk , stirring constantly , until the mixture is very thick . add the egg and milk . add the milk and stir until the mixture is very smooth . add the egg and milk . add the milk and stir until melted . add the milk and stir until melted . add the egg and milk . add the egg and cook until the onions are soft peaks . add the milk , stirring constantly . add the egg and milk . stir until thoroughly incorporated . add the egg and milk . stir until thoroughly combined . add the egg and milk . add the egg and milk . stir\n",
      "\n",
      "> 1    loaf (1 pound) italian 2 tb red wine vinegar bread, cut into 1-inch           1/3 c  olive oil thick slices                     1/4 c  minced basil 2 lg tomatoes, cut into medium salt and ground black pepper dice\n",
      "= dip each slice of bread into a large bowl of cold water , then transfer to a colander set aside in the sink ; let drain for 15 minutes . gently squeeze each slice of bread to remove excess water , then cut the slices into medium dice . transfer bread cubes to a large nonreactive bowl . add the remaining ingredients including 1 teaspoon salt and 1/4 teaspoon pepper ; toss the ingredients to combine . cover and chill for 1 hour . serve .\n",
      "< in a small saucepan , combine the wine , salt , pepper , and garlic . add the bouquet garni , stirring constantly . add the tomatoes and cook for another 5 minutes . add the tomatoes and cook for 5 minutes . add the tomatoes and cook for another 5 minutes . add the tomatoes and cook for another 5 minutes . add the tomatoes and cook for another 5 minutes . add the tomatoes and cook for another 5 minutes . serve with the tortilla chips . <EOS>\n",
      "\n",
      "> 3/4 c  oil 1 c  date sugar 3/4 c  sorghum 1/3 c  soy milk 2 ts vanilla 2 1/2 c  flour 1/2 ts baking soda 1 ts salt 3 c  rolled oats 1/2 c  raisins 3/4 ts cinnamon\n",
      "= mix oil , sweeteners , soy milk & vanilla in a large bowl . ensure that they are well blended & smooth . add remaining ingredients & mix well . bake at 350f for 10 minutes , or until the undersides start to turn brown .\n",
      "< mix the flour , sugar , baking powder , cinnamon , cinnamon , baking powder , cinnamon , cinnamon , cinnamon , and salt in a large bowl . add the eggs , one at a time , beating well after each addition . add the milk and vanilla . mix well . add the flour , cinnamon , raisins , cinnamon , cinnamon , cinnamon , cinnamon , cinnamon , and vanilla . mix well . add the milk and vanilla . mix well . add the vanilla and nuts . drop by teaspoonfuls onto greased baking sheets . bake in a preheated oven for 20 minutes . <EOS>\n",
      "\n",
      "> 2 lg flank steaks 2 tb lemon juice 1 c  dry white wine 2 tb brown sugar or honey 1/2 c  soy sauce 10 1/2 oz canned beef broth 1 1/2 tb minced onion 1    gingerroot piece (1\") 1/4 ts minced garlic - crushed\n",
      "= slice steaks diagonally across grain into 1/4 x 1-inch strips . place in large bowl . mix wine , soy sauce , onion , garlic , lemon juice , brown sugar , broth and gingerroot . pour over meat and marinate 1 hour , turning 3 or 4 times . thread on skewers and grill over coals until done as desired .\n",
      "< in a large skillet , heat oil and brown sugar and brown sugar . add the onion and stir-fry until browned . add the remaining ingredients , stir well . add the wine , honey , honey , honey , honey , honey , honey , honey , honey , honey , soy sauce , lemon juice , honey , honey , honey , honey , honey , honey , honey , honey , soy sauce , lemon juice , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , honey , soy sauce , honey , lemon juice , honey , honey , soy sauce , and ginger . stir well . add the sauce and stir until mixture thickens . add the remaining ingredients except the beef and\n",
      "\n",
      "> 1/4 c  ketchup 1 c  tomato juice 1/4 c  vinegar 1/2 c  water 2 ts worcestershire sauce 1/4 ts chili powder 1/4 ts garlic salt 1 ts paprika 1/8 ts ground cayenne pepper 1 ts dry mustard 1 ts salt 2 ts brown sugar 1/2 c  chopped onion\n",
      "= combine all ingredients and simmer 15 minutes . sufficient for 3 lbs . of ribs . sauce was the creation of my late father-in-law . enjoy !\n",
      "< in a large bowl , combine all ingredients , except the vinegar . cover and chill . <EOS>\n",
      "\n",
      "> 8 oz neufchatel cheese 2    eggs 1 c  undiluted evaporated milk 1/2 c  sugar 3 tb hazelnut liqueur 2 tb flour 3 c  sliced fruit, fresh or- canned 2 tb apricot preserves 2 ts water 3 tb chopped oregon hazelnuts or sliced oregon hazelnuts 1 1/4 c  flour 1/4 c  finely chopped hazelnuts (oregon hazelnuts) 2 tb sugar 1/2 c  chilled butter 1    egg yolk 4 tb ice water\n",
      "= to make pastry , combine flour , hazelnuts and sugar in medium bowl . cut in butter . beat together egg yolk and ice water ; stir into flour mixture . form into a ball . roll dough on floured surface with floured rolling pin . transfer to 9-inch pie plate ; trim and flute edges . for filling , place cheese , eggs , milk , sugar , liqueur and flour in a blender container . cover ; blend until smooth . pour into unbaked hazelnut pastry . bake at 325 for 40 minutes , or until tests done . chill . layer fruit over cheesecake . stir together preserves and water ; brush over fruit . sprinkle with hazelnuts . hazelnut industry and the hazelnut marketing board\n",
      "< combine flour , baking powder , salt , and sugar in a large bowl . add the flour and beat until well blended . add the flour and mix well . add the flour , baking powder , salt , and vanilla . add the bouquet garni , stirring well . add the flour , baking powder , salt and pepper . stir in the flour mixture , add the flour , baking powder , and salt . add the flour , baking powder , salt , and vanilla . stir in the flour mixture , and add the flour mixture . add the flour and mix well . add the flour , baking powder , salt , and vanilla . beat well . add the flour and cook until the mixture is well blended . add the flour and the nuts . add the flour mixture , stirring until\n",
      "\n",
      "> 1 oz fresh yeast; -=or=- 2 pk - instant dry yeast granules 1/2 c  milk; at room temperature 3/4 c  warm water (125f) 1/2 ts salt 3 c  unbleached white flour\n",
      "= in a large bowl , dissolve the yeast in a mixture of the milk and water . mix in the flour and salt to make a soft dough . knead 10 minutes by hand , or 4 minutes in a heavy-duty mixer with a dough hook . oil a clean bowl and set the dough in it , cover tightly with plastic wrap , and let rise until doubled in bulk . -rrb- if the dough is ready sooner than it is needed , punch it down and refrigerate it or let it rise slowly again at room temperature until ready to use . when ready to use , divide into 4 equal parts . roll 1 into an 8-inch round . top with favorite toppings and bake 10 minutes in 450f oven .\n",
      "< in a large bowl , combine the yeast , salt , water , and salt . stir in the flour and the flour . add the yeast and stir until melted . add the flour and mix well . add the flour and mix well . add the flour and mix well . stir in the flour and the yeast . stir in the flour and knead until smooth . add the flour and stir until mixture thickens . add the flour , yeast , salt and vanilla . stir in the flour and knead until smooth . add the flour and mix well . add the flour and yeast . stir in the flour and knead again . stir in the flour and the flour . add the yeast and stir until it is well blended . add the flour and mix well . add the flour and yeast\n",
      "\n",
      "> 1 tb butter 1 tb flour 1 c  milk 1/4 ts salt 1/8 ts pepper\n",
      "= melt butter and add the flour and stir until blended . slowly add the milk and cook until mixture thickens , stirring constantly . add seasonings and cook 2 minutes longer .\n",
      "< in a large bowl , combine flour , baking powder , salt , baking powder , salt , pepper , and salt . add the milk and butter . add the flour , salt , pepper , and salt . stir in the flour , salt , pepper , and nutmeg . add the milk and cook until the onions are soft , about 10 minutes . add the flour and cook until the onions are soft . add the flour , salt , pepper , and salt . cook over low heat for 1 minute more . add the flour , salt , and pepper . cook for another 5 minutes . add the milk and cook until the onions are tender , about 5 minutes . add the flour , stirring constantly . cook over low heat for 30 minutes . add the milk and cook until the\n",
      "\n",
      "> 4 lb onions, sliced or diced 4 oz vegetable oil 2 oz white vinegar 4 tb hungarian paprika 1 ts marjoram, ground 5 lb beef chuck, cubed 2 ts fresh garlic, minced 1 ts caraway seeds, ground 1 x  bay leaf 1 tb lemon zest 1 x  salt to taste\n",
      "= saute onions in vegetable oil , browning well . add vinegar , deglazing lightly . add all other ingredients , except meat ; blend well . add meat . cover and simmer slowly until meat is tender . serve with spaetzle .\n",
      "< in a large pot , combine the bacon , garlic , salt , pepper , and salt . add the bacon and cook until the onions are soft . add the onions and cook until the onions are soft . add the onions , and cook until the onions are soft , about 10 minutes . add the onions and cook until the meat is browned and the onions are clear . add the onions and cook until the meat is very soft . add the onions and cook until the meat is tender . add the onions and cook until the meat is brown . add the onions and cook for another 5 minutes . add the beef and cook for another 5 minutes . add the beef and cook until the vegetables are tender , about 10 minutes . add the beef and cook for a few minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly_attn(baseline2_encoder, attn_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93235ea-8dcd-462d-945e-712849d4c172",
   "metadata": {},
   "source": [
    "**Extension 1: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dc6c726-d186-4327-b401-9b31865fdc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 101340 sentence pairs\n",
      "Trimmed to 79894 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Ingredients 17999\n",
      "Recipe 31231\n",
      "['cream milk egg yolk sugar shot jack daniels dark chocolate melted for chocolate syrup sugar water cocoa powder for soda soda water or until glass is full shot jack daniels', 'to make the ice cream bring milk and cream to a boil in a separate bowl mix the yolks with the sugar then stir in of the milk mixture stir well then add the rest of the milk strain through a fine strainer and add melted chocolate and jack daniel s for the sauce bring water and sugar to a boil then stir in cocoa let cool in a ounce pilsner glass pour in tablespoons syrup then top with scoop ice cream repeat pour in jack daniel s and fill with soda water serve with long dessert spoon and a straw courtesy of john villa of judson grill']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 150\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # Define regular expression pattern to match numeric values and units\n",
    "    amount_pattern = r'\\b[\\d.]+(?:\\s*(?:g|kg|l|ml|tsp|tbsp|cup|pint|quart|oz|c|tb|ts|b|ea|pk|t|can|cups)\\b)?'\n",
    "    # Remove numeric values and units from the text\n",
    "    text = re.sub(amount_pattern, '', text)\n",
    "\n",
    "    # Remove irrelevant information like advertisements, website navigation elements, etc.\n",
    "    # Assuming such information may be enclosed within angle brackets or parentheses\n",
    "    bracket_pattern = r'<[^>]*>|(\\([^)]*\\))'\n",
    "    remove_words = r'-rrb-|-lrb-'\n",
    "    # Remove text inside angle brackets and parentheses\n",
    "    cleaned_text = re.sub(bracket_pattern, '', text)\n",
    "    cleaned_text = re.sub(remove_words, '', cleaned_text)\n",
    "\n",
    "    # Handle missing data and standardize text format\n",
    "    cleaned_text = cleaned_text.lower()  # Convert text to lowercase\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)  # Remove punctuation\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra whitespaces\n",
    "    return cleaned_text\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, train_pairs, valid_pairs, test_pairs = readLangs(reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(train_pairs))\n",
    "    train_pairs = filterPairs(train_pairs)\n",
    "    valid_pairs = filterPairs(valid_pairs)\n",
    "    test_pairs = filterPairs(test_pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(train_pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in train_pairs:\n",
    "        pair[0] = clean_text(pair[0])\n",
    "        pair[1] = clean_text(pair[1])\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in valid_pairs:\n",
    "        pair[0] = clean_text(pair[0])\n",
    "        pair[1] = clean_text(pair[1])\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    for pair in test_pairs:\n",
    "        pair[0] = clean_text(pair[0])\n",
    "        pair[1] = clean_text(pair[1])\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, train_pairs, valid_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, valid_pairs, test_pairs = prepareData('ingredients', 'recipe', False)\n",
    "print(random.choice(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7112d21-9cfc-4bcd-a2bf-2d46b78f0413",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plot_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 5\u001b[0m extension1_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m extension1_decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m trainIters_attn(extension1_encoder, extension1_decoder, \u001b[38;5;241m10000\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "n_iter = 10000\n",
    "plot_every = 100\n",
    "print_every = 100\n",
    "extension1_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "extension1_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "trainIters_attn(extension1_encoder, extension1_decoder, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fac582-4b7a-4825-a9ec-54624622a757",
   "metadata": {},
   "source": [
    "**Extension 2: Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20845898-86b7-4d01-af5e-aca296af2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, pretrained_embeddings=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Load pretrained embedding if any\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3bc12e2-1e16-4329-8e74-c1faa38e5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH, pretrained_embeddings=None):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load pretrained embedding if any\n",
    "        if pretrained_embeddings is not None:\n",
    "            # Assuming pretrained_embeddings is a numpy array containing Word2Vec embeddings\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf291a29-bb39-4b29-b8fb-2a05c055f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4f0d5f8-bdc4-4da5-9f4e-0e0bb3dcee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e5af9df-aa3c-4983-a416-373209ab5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = word2vec_model.vector_size\n",
    "# Initialize an empty numpy array for pretrained embeddings\n",
    "pretrained_embeddings = np.zeros((len(input_lang.unique_words), embedding_size))\n",
    "# Populate the numpy array with embeddings\n",
    "for i in range(len(input_lang.unique_words)):\n",
    "    word = input_lang.unique_words[i]\n",
    "    if word in word2vec_model:\n",
    "        pretrained_embeddings[i] = word2vec_model[word]\n",
    "    else:\n",
    "        # Handle out-of-vocabulary words (e.g., initialize with zeros)\n",
    "        pretrained_embeddings[i] = np.zeros(embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83f0d552-99b8-4fd8-9774-5170f5b7d95d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m----> 2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, embedding_size, hidden_size, pretrained_embeddings\u001b[38;5;241m=\u001b[39mpretrained_embeddings)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(embedding_size, hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, pretrained_embeddings\u001b[38;5;241m=\u001b[39mpretrained_embeddings)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m trainIters(encoder, decoder, \u001b[38;5;241m10000\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5217\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "decoder = AttnDecoderRNN(embedding_size, hidden_size, output_lang.n_words, dropout_p=0.1, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "trainIters(encoder, decoder, 10000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5c256-36c8-4a1a-a8f5-186241e6fabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
