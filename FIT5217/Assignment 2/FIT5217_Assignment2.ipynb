{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3712cd",
   "metadata": {},
   "source": [
    "**Neural Chef Assistance**  \n",
    "**Name: Darren Jer Shien Yee**  \n",
    "**Student ID: 31237223**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e2844",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: RNN without Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7c08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Requirements\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d788a9e-58f9-4882-b4b6-5bf045a8c55c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Language Method imported from RNN code provided in tutorials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b0337d-7e9f-474f-bbd1-83b5c9b887c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 3: \"NUM\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3c46-345e-4f6b-9fff-cff0800cd43c",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Unicode and ASCII method to preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d787a58e-58ae-48f3-956c-fc7f98282e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "# Lowercase only (since numerical is crucial here as opposed to lab code)\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.replace('\\t', ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeb7d6-145e-4024-842e-ebc28427ad01",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Method to read data from provided CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec3f0e9-c616-41d4-92d6-c0e3f6cac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def readRecipeData(file_path, reverse=False):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.fillna('', inplace=True)\n",
    "    ingredients = df['Ingredients']\n",
    "    recipes = df['Recipe']\n",
    "    pairs = [[normalizeString(ing), normalizeString(rec)] for ing, rec in zip(ingredients, recipes)]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('Recipe')\n",
    "        output_lang = Lang('Ingredients')\n",
    "    else:\n",
    "        input_lang = Lang('Ingredients')\n",
    "        output_lang = Lang('Recipe')\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372752f-bdba-432c-9fa7-5fd32712487a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Preprocess data and intialise language using the methods above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d52048f-342f-49c6-88ed-d2d48bf896c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fractions import Fraction\n",
    "# def is_number(word):\n",
    "#     try:\n",
    "#         float(word)\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         return False\n",
    "\n",
    "# def replace_numbers_with_token(sentence, token='NUM'):\n",
    "#     # Define a regular expression pattern to match numbers and fractions\n",
    "#     number_pattern = r'([\\d]+(?:[.//-][\\d]+)?|[\\d]+-[\\d]+)'\n",
    "#     # Define a regular expression pattern to match text inside angle brackets and parentheses\n",
    "#     bracket_pattern = r'<[^>]*>|(\\([^)]*\\))'\n",
    "#     remove_words = r'-rrb-|-lrb-'\n",
    "#     # Replace numbers and fractions with the specified token using regular expression\n",
    "#     replaced_sentence = re.sub(number_pattern, token, sentence)\n",
    "#     # Remove text inside angle brackets and parentheses\n",
    "#     replaced_sentence = re.sub(bracket_pattern, '', replaced_sentence)\n",
    "#     replaced_sentence = re.sub(remove_words, '', replaced_sentence)\n",
    "#     return replaced_sentence\n",
    "\n",
    "# def prepareData(reverse=False, max_ingredient=150, max_recipe=150):\n",
    "#     input_lang, output_lang, pairs = readRecipeData('Cooking_Dataset/Cooking_Dataset/train.csv', reverse)\n",
    "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     print(\"Counting words...\")\n",
    "#     new_pairs = []\n",
    "#     for pair in pairs:\n",
    "#         # Replace numbers with NUM token in both input and output sentences\n",
    "#         pair[0] = replace_numbers_with_token(pair[0])\n",
    "#         pair[1] = replace_numbers_with_token(pair[1])\n",
    "#         input_words = pair[0].split()\n",
    "#         output_words = pair[1].split()\n",
    "#         if len(input_words) < max_ingredient and len(output_words) < max_recipe:\n",
    "#             # Add preprocessed pair to language objects\n",
    "#             input_lang.addSentence(pair[0])\n",
    "#             output_lang.addSentence(pair[1])\n",
    "#             new_pairs.append(pair)\n",
    "#     print(\"Counted words:\")\n",
    "#     print(input_lang.name, input_lang.n_words)\n",
    "#     print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "#     return input_lang, output_lang, new_pairs\n",
    "\n",
    "\n",
    "# input_lang, output_lang, pairs = prepareData(False,max_ingredient=150, max_recipe=150)\n",
    "# print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add1f86a-2221-4fcd-a3ce-9181f68d197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 101340 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Ingredients 36575\n",
      "Recipe 35181\n",
      "['3 lg carrots 1 lg turnip 2    stalks celery 2 lg onions 2 tb butter 3 qt water 2 ts salt 6 lg sprig parsley 1/2    bay leaf 1 ts thyme leaves', 'scrub and coarsley chop carrots , turnip , celery , peel and chop onions . melt butter in 8-quart pan over medium heat . add chopped vegetables and cook , stirring occasionally , until vegetables turn golden -lrb- about 15 minutes -rrb- . add water , salt , parsley , bay leaf , and thyme leaves . cover and bring to a boil . reduce heat and simmer for 1-1/2 hours . strain and discard vegetables .']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 150\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "    \n",
    "def prepareData(reverse=False):\n",
    "    input_lang, output_lang, pairs = readRecipeData('Cooking_Dataset/Cooking_Dataset/train.csv', reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    new_pairs = []\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(False)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa22b6-5890-459f-a0de-eca4d17645d0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Encoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad39fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170834eb-3f3b-466c-9fd2-76f05695b79a",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Decoder RNN for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504668e0-5099-4670-b2b8-3688377eee99",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train method for seq2seq model without attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93e74aa1-b285-459a-ae49-fb99cfd3d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e73517-c02f-44d4-a0e5-e45f6c0a3be3",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Time method for train iter method** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30717ca1-4e2b-4a59-bd84-5eb1e68ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90864fb4-1563-4b30-9e67-d3da22fe4ad2",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Processing methods for train iter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07926753-8ce7-46d0-8237-c9ddce9b8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96969eea-8ad7-4879-8fec-2e72756ae498",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Show Plot method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70a09497-62ff-467c-8923-87e52b92fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "    \n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c1ccc-4c8a-4e72-b752-f0a12edf9cb0",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Train iter method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8c7ce3-911c-49f9-be91-524a542e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdb0fe-aac9-48f4-b1fb-2e500d5b9c73",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Creating instance of baseline 1 and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d95a1d-094a-44e8-a874-246e81ff79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "baseline1_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "baseline1_decoder = DecoderRNN(hidden_size,output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(baseline1_decoder, baseline1_decoder, 20000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b9c14-0d88-4401-84f6-3641172bab96",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 1: Evaluation methods to check performance of baseline 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05211516-e976-47d1-b914-449d0d3fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=150):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbea66-faee-4cb0-b11a-bb5312664000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words= evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80904440-930c-4261-9bd6-f44f27e5f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(baseline1_encoder, baseline1_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c35846-ac86-452b-9b8c-54f465a1f9ba",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "814880f7-ef67-460c-980c-09b6745faa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=150):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        #self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        attn_weights = F.softmax(torch.bmm(hidden, encoder_outputs.T.unsqueeze(0)),dim=-1)\n",
    "        attn_output = torch.bmm(attn_weights, encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "\n",
    "        # attn_weights = F.softmax(\n",
    "        #     self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "        #                          encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        # output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # output = F.relu(output)\n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f95b7f-3c49-4a84-ade1-3242c219489f",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7f5d38e-94a3-47d4-b987-b9769d95ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "def train_attn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=150):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486009e-c90f-401d-ab3e-0423519d0b10",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Train iter method for RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4ac4ebb-e010-4984-97d3-5b223b228062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters_attn(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        valid_pair_found = False\n",
    "        while not valid_pair_found:\n",
    "            training_pair = random.choice(training_pairs)\n",
    "            input_tensor, target_tensor = training_pair\n",
    "\n",
    "            if input_tensor.size(0) <= 150 and target_tensor.size(0) <= 150:\n",
    "                valid_pair_found = True\n",
    "            else:\n",
    "                continue\n",
    "        loss = train_attn(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942fc9b-e5f6-41cf-af02-116ca14b1874",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Initialising instance of RNN with attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "735b4dee-5b75-4325-ae4a-267a1fb93592",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m----> 2\u001b[0m baseline2_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m baseline2_decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m trainIters_attn(baseline2_encoder, baseline2_decoder, \u001b[38;5;241m10000\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "baseline2_encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "baseline2_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters_attn(baseline2_encoder, baseline2_decoder, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23e78a-7de0-4a1e-8f37-23c8d1d05668",
   "metadata": {},
   "source": [
    "**Implementation of Baseline 2: Evaluation method for RNN with Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ce082-cf7e-471b-9488-615f4ddc8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attn(encoder, decoder, sentence, max_length=200):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af3899-43bc-409e-8303-d139e8d108c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly_attn(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attention= evaluate_attn(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "48da6244-ed20-456d-bcfc-f3494ab6e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> NUM lb peaches, small NUM c  water NUM lb sugar NUM ts cinnamon, ground NUM NUM c  vinegar, cider cloves, whole\n",
      "= mrs. donald e. jefferson , caroline co. , md , per the hammond-harwood house cookbook pickled peaches peel peaches and stick a clove in each . mix the sugar , vinegar and water and bring to a boil . cook until a straw pushes easily through to the pit . pack into hot sterilized jars , fill with hot syrup and seal the jars . process NUM minutes in a boiling water bath .\n",
      "< in the . water in . . . the . . . . . , . . . . . and . . . NUM . . . . . and . to . NUM the . . . . . . . <EOS>\n",
      "\n",
      "> NUM    yeast cak NUM NUM c  lukewarm water NUM c  flour NUM tb sugar NUM tb salt NUM c  water NUM c  brown sugar NUM tb shortening NUM c  wheat flour\n",
      "= soften yeast in the lukewarm water . add NUM cups flour , sugar , and salt ; beat until smooth . let rise until bubbly . combine remaining water , brown sugar , and shortening . heat until shor tening melts . cool , add to yeast mix . add wheat flour . mix until smooth . knead NUM minutes . place in a greased bowl , let rise NUM hours . old standby .\n",
      "< in the water . the . . . . NUM . . . . . . . . . <EOS>\n",
      "\n",
      "> NUM ea chicken breasts, skin, split NUM c  minced onion NUM t  paprika NUM t  salt NUM t  rosemary NUM t  pepper NUM t  flour NUM c  orange juice\n",
      "= sprinkle with onion and seasonings . blend flour with NUM c orange juice , stir in the remaining juice and pour over the chicken . bake , uncovered , basting occasionally at NUMf for NUM hr . or until tender . serve the chicken over noodles or rice on a warm platter . stir the pan juices to blend and pour over the chicken .\n",
      "< in the juice the . add , and . . . the . . . . NUM and . to . add the . . . . . , . . . . . . . . NUM , and . . . <EOS>\n",
      "\n",
      "> NUM c  unbleached flour NUM tb olive oil NUM c  semolina or durum flour NUM    eggs ; beaten NUM ts salt\n",
      "= place all ingredients except egg into a food processor . add the egg a bit at a time while pulsing . when the dough forms into pellets , you 've added enough eggs . turn the dough onto a work surface and knead together into a stiff mass . let rest for NUM minutes before rolling into pasta .\n",
      "< in the . oil the . in . . . . . NUM the . . . . . . , . . . . . . . . . . the . . . . . . . NUM to the . . . <EOS>\n",
      "\n",
      "> NUM md cabbage head NUM tb salt NUM md onions, sliced in thirds NUM ts pepper NUM c  sugar NUM ts celery salt NUM c  oil NUM ts dry mustard NUM c  cider vinegar\n",
      "= in ceramic dish , layer shredded cabbage with onions . sprinkle sugar over the layers . whisk together and bring to boil the oil , vinegar , salt , pepper and other spices . pour mixture over cabbage ; seal tightly and refrigerate NUM hours . mix well before serving . substitute NUM bell peppers for one of the onions .\n",
      "< in the and the . and . , . to salt pepper . . , . . and . . NUM . and . to . . NUM . and . . NUM . and . the . . . . . . . . and . . NUM and . <EOS>\n",
      "\n",
      "> NUM g  canned flageolets NUM ts ground cumin NUM ts coriander NUM ts turmeric NUM lg garlic clove; crushed NUM    lemon; juiced NUM tb olive oil or slightly more salt\n",
      "= put the flageolets , spices , garlic and lemon juice into the blender and whizz to a thick pur e with some of the juices from the can . gradually stir the oil into the mixture to thin out to a dipping consistency . season to taste .\n",
      "< in the oil in a large skillet , saute the garlic and . oil in . oil until the . add the . . . . are are the . . . . . . . . . NUM and . . . . . . NUM to . <EOS>\n",
      "\n",
      "> NUM c  water NUM tb onion NUM tb celery, sliced; or tops NUM c  flour NUM tb catsup NUM    cl garlic; minced salt and pepper lard or shortening\n",
      "= enjoy . add shortening to large pan . saute minced garlic . dip pieces of meat in flour , salt and pepper mixture . fry in hot fat . drop in kettle of water -lrb- NUM cups  and add remaining ingredients . adapts to crockpot cooking .\n",
      "< in the the . water the . . . . . and . the . . . . . , . . . . to add the . . . . . NUM . and . . to . . . NUM and . the . . . . . . . NUM to . . . the NUM and . . and . . . <EOS>\n",
      "\n",
      "> NUM    NUM\" graham cracker crust NUM pk NUM oz softened cream cheese NUM c  NUM oz. sweetened condensed; milk  NUM c  concentrated lemon juice NUM ts vanilla extract NUM c  cherry pie filling\n",
      "= in a large mixer bowl , beat cheese until fluffy . beat in condensed milk until smooth . stir in lemon juice and vanilla . pour into crust . chill NUM hours or until set . top with desired amount of e filling before serving . refrigerate leftovers .\n",
      "< in the juice in . a . the . . . . . . NUM . . . . . . . the . . . . . . . <EOS>\n",
      "\n",
      "> NUM c  diced potatoes NUM c  boiling water NUM lb fish fillets, cubed NUM ts salt NUM ts pepper NUM tb parsley, chopped NUM sl bacon NUM    medium onion, chopped NUM c  milk\n",
      "= place potatoes in dutch oven with boiling water , cover and cook NUM-NUM minutes . fry bacon until transparent , add onion and cook until onion is soft and bacon is lightly browned . add bacon , onion , bacon drippings and fish fillets to potatoes . simmer NUM minutes or until potatoes and fish are done . stir in milk , salt and pepper . simmer NUM minutes . sprinkle with parsley .\n",
      "< cook the . in . , . add the . . . . and . stir to NUM , . . . and . . . . the . . . . <EOS>\n",
      "\n",
      "> NUM NUM c  sugar NUM c  rolled oats NUM c  flour NUM ts baking powder NUM ts salt NUM NUM c  shortening NUM ts baking soda NUM c  boiling water NUM c  bran flakes\n",
      "= add soda to boiling water and let stand until cool . mix together flour , baking powder , salt , bran flakes , rolled oats and sugar . cut in shortening ; add water and soda . roll outthin on a floured board . bake in hot oven until golden brown .\n",
      "< sift together the flour . add the . . . . . . , . . . . . bake at NUM degrees . and for and the . and . NUM and . and . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly_attn(baseline2_encoder, baseline2_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91b0dd-afa1-41d8-a377-91b05f788c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN_Layers(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(EncoderRNN_Layers, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53967f-1805-48f6-93e8-b647fa319d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN_Layers(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,num_layers, dropout_p=0.1, max_length=200):\n",
    "        super(AttnDecoderRNN_Layers, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "        self.iteration = 0\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        self.iteration += 1\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        attn_weights = F.softmax(torch.bmm(hidden[-1].unsqueeze(0), encoder_outputs.T.unsqueeze(0)),dim=-1)\n",
    "        attn_output = torch.bmm(attn_weights, encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
    "\n",
    "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b2e68-f1e1-4bb8-bf0c-bf1939856774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attn(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=200):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # Initialize decoder hidden state for each layer\n",
    "    decoder_hidden = encoder_hidden.new_zeros(decoder.num_layers, 1, decoder.hidden_size)\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di].unsqueeze(0)  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "c42bc97d-0748-4cbf-823e-37b22e5422bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters_attn_layers(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        valid_pair_found = False\n",
    "        while not valid_pair_found:\n",
    "            training_pair = random.choice(training_pairs)\n",
    "            input_tensor, target_tensor = training_pair\n",
    "\n",
    "            if input_tensor.size(0) <= 150 and target_tensor.size(0) <= 150:\n",
    "                valid_pair_found = True\n",
    "            else:\n",
    "                continue\n",
    "        loss = train_attn(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "4388ee78-5d3b-4309-89d5-27fb9d0694b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (3, 1, 256), got [2, 1, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[452], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m more_layers_encoder \u001b[38;5;241m=\u001b[39m EncoderRNN_Layers(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size,num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m more_layers_decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN_Layers(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words,\u001b[38;5;241m3\u001b[39m, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m trainIters_attn_layers(more_layers_encoder, more_layers_decoder, \u001b[38;5;241m50000\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[451], line 23\u001b[0m, in \u001b[0;36mtrainIters_attn_layers\u001b[1;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_attn(input_tensor, target_tensor, encoder,\n\u001b[0;32m     24\u001b[0m              decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0;32m     25\u001b[0m print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     26\u001b[0m plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[1;32mIn[428], line 28\u001b[0m, in \u001b[0;36mtrain_attn\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_teacher_forcing:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Teacher forcing: Feed the target as the next input\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[1;32m---> 28\u001b[0m         decoder_output, decoder_hidden, decoder_attention \u001b[38;5;241m=\u001b[39m decoder(\n\u001b[0;32m     29\u001b[0m             decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0;32m     30\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(decoder_output, target_tensor[di])\n\u001b[0;32m     31\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m target_tensor[di]  \u001b[38;5;66;03m# Teacher forcing\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[344], line 20\u001b[0m, in \u001b[0;36mAttnDecoderRNN_Layers.forward\u001b[1;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded)\n\u001b[1;32m---> 20\u001b[0m _, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(embedded, hidden)\n\u001b[0;32m     21\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mbmm(hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), encoder_outputs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights, encoder_outputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1131\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1134\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:277\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    275\u001b[0m expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fit5201\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:260\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    258\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden size (3, 1, 256), got [2, 1, 256]"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "more_layers_encoder = EncoderRNN_Layers(input_lang.n_words, hidden_size,num_layers = 2).to(device)\n",
    "more_layers_decoder = AttnDecoderRNN_Layers(hidden_size, output_lang.n_words,3, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters_attn_layers(more_layers_encoder, more_layers_decoder, 50000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51187e4-277e-4bca-b429-b433ea236d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly_attn(more_layers_encoder, more_layers_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef54a4-3d17-4f09-b77c-ebe2bd69153b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
