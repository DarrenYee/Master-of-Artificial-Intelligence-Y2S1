{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Activity 3.1 Perceptron\n",
    "(last modified 27 Mar 2023)\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "In this activity, you learn how to \n",
    "\n",
    "- generate synthetic Gaussian mixture data to test classification algorithms\n",
    "- implement the Perceptron algorithm\n",
    "- evaluate and describe weaknesses of the Perceptron\n",
    "- compute and plot the decision boundary for 2-dimensional input data\n",
    "- modify linear classification algorithms to allow for an affine decision boundary (one that does not run through the origin)\n",
    "\n",
    "In you Assignment, you will be asked to expand the Perceptron algorithm to multi-class classification problems.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Module 1\n",
    "- Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data for Classification\n",
    "\n",
    "In this and other activities of Module 3, we again work with synthetic data generators, which makes it easier to study the model we develop and validate the obtained results. In particular, we are working **Gaussian mixture models** where the input data for each class follows a (multivariate) Gaussian distribution and the classes follow marginal some categorical distribution. Formally, we can define the distribution of our target variable $T \\in \\{0, 1, \\dots, k-1\\}$ and our input variable(s) $X \\in \\R^p$ as\n",
    "\\begin{align*}\n",
    "T &\\sim \\mathrm{Cat}(\\varphi_1, \\dots, \\varphi_k)\\\\\n",
    "X | T &\\sim N_p(\\boldsymbol{\\mu}_T, \\Sigma_T)\n",
    "\\end{align*}\n",
    "where $\\varphi_1, \\dots, \\varphi_k \\in [0, 1]^k$ are the marginal class probalities with $\\sum_{l=1}^{k}\\varphi_l = 1$ and for each $l \\in \\{1, \\dots, k\\}$, $\\boldsymbol{\\mu}_l \\in \\R^p$ and $\\boldsymbol{\\Sigma}_l \\in \\R^{p \\times p}$ are the mean vector and the covariance matrix of the inputs with class $l$.\n",
    "\n",
    "Don't worry if you don't yet know about the multivariate Gaussian distribution. We will talk more about it next week. In particular, in this activity we will work with unit covariance matrices in two dimensions, i.e.,\n",
    "\\begin{equation*}\n",
    "\\Sigma = \n",
    "\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "which means that the two input variables $X=(X_1, X_2)$ are independent and normally distributed according,\n",
    "$$X_i | T \\sim N(\\mu_{T, i}, 1)$$\n",
    "\n",
    "Concretely, in this activity we are working with the following parameters:\n",
    "- the marginal class probabilities are $(\\phi_1, \\phi_2)=(0.6, 0.4)$\n",
    "- the mean vector of the input variables for class $1$ is $(3.5, 2.5)$, i.e., $X_1 | T = 1 \\sim N(3.5, 1)$ and $X_2 | T = 1 \\sim N(2.5, 1)$\n",
    "- the mean vector of the input variables for class $2$ is $(0, 6)$, i.e., $X_1 | T = 2 \\sim N(0, 1)$ and $X_2 | T = 2 \\sim N(6, 1)$\n",
    "\n",
    "Let us implement this distribution and draw and plot some training data to get an intuition about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_gaussian_mixture_data(n, means, covs=None, class_probs=None, random_state=None):\n",
    "    RNG = np.random.default_rng(seed=random_state)\n",
    "    d = len(means[0])\n",
    "    k = len(means)\n",
    "\n",
    "    # sample outputs\n",
    "    # if no class probabilities are provided, assume uniform\n",
    "    class_probs=np.ones(k)/k if class_probs is None else class_probs\n",
    "\n",
    "    # generate the y-sample using a multinomial distribution with 'number of experiments' equal to 1;\n",
    "    # this results in a categorical distribution\n",
    "    # the output of multinomial is a n times x binary matrix with a single 1-entry per row indicating\n",
    "    # what class that row belongs to; we map this to the numbers 0 to (k-1) with np.nonzero\n",
    "    _, y = np.nonzero(RNG.multinomial(1, class_probs, size=n))\n",
    "\n",
    "    # sample inputs conditioned on outputs\n",
    "    # if no covariances are provided assume unit\n",
    "    covs = [np.eye(d) for _ in range(k)] if covs is None else covs\n",
    "    x = np.zeros(shape=(n, d))\n",
    "    for i in range(k):\n",
    "        idx_i = np.flatnonzero(y==i)\n",
    "        x[idx_i] = RNG.multivariate_normal(means[i], covs[i], size=len(idx_i))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "p0 = 0.60; p1 = 1 - p0\n",
    "class_probs = [p0, p1] \n",
    "mu0 = np.array([3.5, 2.5])\n",
    "mu1 = np.array([0.0, 6.0])\n",
    "\n",
    "make_gaussian_mixture_data(10, [mu0, mu1], class_probs=class_probs, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task A: Generate and Plot Training Data\n",
    "\n",
    "Let us generate 100 training data points and plot them in a scatter plot using a different color for each class. Most of the code for this is already complete. We just need to select the individual columns and, for each call of `scatter`, the rows that belong to one specific class.\n",
    "\n",
    "**Complete the two lines below to select the indiviudal data columns `0` and `1` and within those only the data points of the specific class `c`**\n",
    "\n",
    "Hint: Remember how you can use indexing with a `numpy.ndarray` to select parts of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_data_by_target_value(x, y, ax=None, scatter_params={'ec': 'black', 'alpha': 0.8}):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    for c in range(2):\n",
    "        x1_c = # YOUR CODE HERE\n",
    "        x2_c = # YOUR CODE HERE\n",
    "        ax.scatter(x1_c, x2_c, label=f'$c={c}$', **scatter_params)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "n = 100\n",
    "x_train, y_train = make_gaussian_mixture_data(n, [mu0, mu1], class_probs=class_probs, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "scatter_data_by_target_value(x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Perceptron\n",
    "\n",
    "As introduced in the lecture, the Perceptron is a linear predictive model for binary classification with parameters $\\boldsymbol{w} \\in \\R^p$ defining a **prediction function**\n",
    "$$\n",
    "y(\\boldsymbol{x}, \\boldsymbol{w}) = \n",
    "\\begin{cases}\n",
    "1 &, \\text{ if }\\boldsymbol{w}^T\\boldsymbol{x} \\geq 0\\\\\n",
    "0 &, \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "which can also be written as $y(\\boldsymbol{w}^T\\boldsymbol{x})=f(\\boldsymbol{w}^T\\boldsymbol{x})$ using the non-linear step function \n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "1 &, \\text{ if }a \\geq 0\\\\\n",
    "0 &, \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "as **activation function**.\n",
    "\n",
    "The **Perceptron algorithm** for fitting the weights $\\boldsymbol{w}$ is an early iterative learning algorithm that can be considered a variant of stochastic gradient descent with batch size $1$. It can be semi-formally described as:\n",
    "\n",
    "- repeat until small training error or maximum number of weight updates reached\n",
    "    - for each training data point $(x_n, t_n)$ (in random order)\n",
    "        - if $y(x_n)\\neq t_n$ then $w \\leftarrow w + (2t_n - 1) \\eta x_n$\n",
    "\n",
    "where $\\eta > 0$ is some appropriately chosen learning rate (like in stochastic gradient descent.)\n",
    "\n",
    "Note that the formula $2t_n - 1$ is just a compact way to write the case distinction\n",
    "$$\n",
    "w \\leftarrow\n",
    "\\begin{cases}\n",
    "w + \\eta x_n &, \\text{ if } t_n = 1\\\\\n",
    "w - \\eta x_n &, \\text{ if } t_n = 0\n",
    "\\end{cases} \\\\\n",
    "$$\n",
    "that is useful to keep the code simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an almost finished implementation of the algorithm. Note that as in our implementation of stochastic gradient descent, we are tracking the full history of the weight vectors for the purpose of visualisation. In addition, we are also tracking the order in which the training examples are considered during training.\n",
    "\n",
    "#### Task B: Implement Predict\n",
    "\n",
    "**Fill in the code for `predict` below that implements $y(\\boldsymbol{x}, \\boldsymbol{w})$ based on the weights `self.w_` that have been set by the `fit` method.**\n",
    "\n",
    "Hints:\n",
    "- Remember that, in `numpy` we can conveniently compute componentwise comparisons with a constant, e.g., `a < 5` computes a boolean array that contains the results of comparing all entries of `a` with `5` (try it out if this is unclear).\n",
    "- You can typecase arrays with the method `astype`; here we want integer (`int`) predictions instead of `bool` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, eta=0.01, tau_max=10000, epsilon=0.005, random_state=None):\n",
    "        self.eta = eta   # learning rate\n",
    "        self.tau_max = tau_max   # max number of iterations\n",
    "        self.epsilon = epsilon   # tolerable error\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        RNG = np.random.default_rng(self.random_state)\n",
    "        n, p = x.shape\n",
    "\n",
    "        self.w_trace_ = np.empty((self.tau_max, p))     # trace of weights during training\n",
    "        self.xy_idx_trace_ = np.empty(self.tau_max, int) # trace of considered training point indices\n",
    "        \n",
    "        # initialisation of iteration counter and weights\n",
    "        tau = 0 \n",
    "        self.w_ = self.w_trace_[0,:] = RNG.normal(size=p) \n",
    "        terminate = False\n",
    "        while not (terminate or (self.predict(x)!=y).mean()<self.epsilon):\n",
    "            # random order to consider training data\n",
    "            order = RNG.choice(np.arange(n),size = n ,replace = False)\n",
    "            for i in range(n):\n",
    "                if self.predict(x[order][i]) != y[order][i]:\n",
    "                    self.w_ = self.w_ + (2*y[order][i]-1)*self.eta * x[order][i]\n",
    "                \n",
    "                self.w_trace_[tau+1] = self.w_\n",
    "                self.xy_idx_trace_[tau] = order[i]\n",
    "\n",
    "                tau +=1\n",
    "                if tau == self.tau_max-1:\n",
    "                    terminate = True\n",
    "                    break\n",
    "        \n",
    "        # remove empty rows from traces\n",
    "        self.w_trace_ = self.w_trace_[:tau] \n",
    "        self.xy_idx_trace_ = self.xy_idx_trace_[:tau-1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialise and train the perceptron with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state=0)\n",
    "perceptron.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let us first look at train and test performance. For the test performance, we can just generate a lot of new data points, exploiting the fact that we have a synthetic data generator.\n",
    "\n",
    "#### Task C: Check Train and Test Performance.\n",
    "\n",
    "Hint: You can use `zero_one_loss` from `sklearn.metrics` or simply compute the mean zero/one error using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "x_test, y_test = make_gaussian_mixture_data(n=1000, means=[mu0, mu1])\n",
    "\n",
    "train_error = # YOUR CODE HERE\n",
    "test_error = # YOUR CODE HERE\n",
    "\n",
    "train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Learned Decision Boundary\n",
    "\n",
    "Let us now visualise the **decision boundary** that the Perceptron has learned, i.e., the hyperplane (a line in the case of 2d-data) of $\\boldsymbol{x}$-values where \n",
    "$$\n",
    "\\boldsymbol{w}^T\\boldsymbol{x}= w_1x_1 + w_2x_2 =0 \\enspace .\n",
    "$$\n",
    "Below we already give a function that can plot a line in matplotlib given in the form $y=ax + b$, i.e., where the $y$-coordinate in the plot is given as a linear function of the $x$-coordinate with a slope $a$ and intercept $b$. To use this function we have to express our decision boundary in this form.\n",
    "\n",
    "#### Task D: Express decision boundary as function $x_2 = ax_1 + b$\n",
    "\n",
    "**Describe the representation in markdown below (or using pen and paper) and implement it in the next code cell.**\n",
    "\n",
    "Hint: The $y$-coordinate in the plot is the $x_2$-coordinate of our input data and the $x$-coordinate in the plot is the $x_1$-coordinate of our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(slope, intercept, ax=None, shape='--', **kwargs):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    x_vals = np.array(ax.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.plot(x_vals, y_vals, shape, **kwargs)\n",
    "\n",
    "def plot_decision_boundary_from_weights(weights, ax=None):\n",
    "    slope = # YOUR CODE HERE\n",
    "    intercept = # YOUR CODE HERE\n",
    "    plot_line(slope, intercept, ax, shape='--', color='red')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "scatter_data_by_target_value(x_train, y_train)\n",
    "plt.arrow(0, 0, *(perceptron.w_*5))\n",
    "plot_decision_boundary_from_weights(perceptron.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Perceptron Learning\n",
    "\n",
    "Next, let us have a look at the learning process of the Perceptron algorithm as captured by the sequence of weight vectors that are attained during training. Let us plot both, the development of the weights as well as that of the training and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = np.array([zero_one_loss((x_test.dot(w)>=0), y_test) for w in perceptron.w_trace_])\n",
    "train_errors = np.array([zero_one_loss((x_train.dot(w)>=0), y_train) for w in perceptron.w_trace_])\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n",
    "axs[1].plot(train_errors, label='train 0/1-error')\n",
    "axs[1].plot(test_errors, label='test 0/1-error')\n",
    "axs[1].set_xlabel(r'$\\tau$')\n",
    "axs[1].legend()\n",
    "axs[0].plot(perceptron.w_trace_[:, 0], label='$w_1$')\n",
    "axs[0].plot(perceptron.w_trace_[:, 1], label='$w_2$')\n",
    "axs[0].set_xlabel(r'$\\tau$')\n",
    "axs[0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task E: Comment on Learning Behaviour\n",
    "\n",
    "**Give a one paragraph summary on what you observe about the learning behaviour. Is there something you are concerned about? How could we potentially improve the Perceptron learning?**\n",
    "\n",
    "Hint: For the last question, you might also want to investigate the next illustration.\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyse the learning, let us pick one training update during which the training error became worse and plot the corresponding decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_diffs = train_errors[:-1] - train_errors[1:]\n",
    "min_idx = np.argmin(train_error_diffs)\n",
    "min_idx, train_errors[min_idx: min_idx+2]\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "axs[0].set_title(f'$\\\\tau={min_idx}$')\n",
    "scatter_data_by_target_value(x_train, y_train, axs[0])\n",
    "axs[0].scatter(x_train[perceptron.xy_idx_trace_[min_idx], 0], x_train[perceptron.xy_idx_trace_[min_idx], 1], ec='red', fc='none', lw=2.5, label='selected')\n",
    "plot_decision_boundary_from_weights(perceptron.w_trace_[min_idx], axs[0])\n",
    "axs[0].arrow(0, 0, *(perceptron.w_trace_[min_idx]*20))\n",
    "axs[0].legend()\n",
    "axs[1].set_title(f'$\\\\tau={min_idx+1}$')\n",
    "scatter_data_by_target_value(x_train, y_train, axs[1])\n",
    "plot_decision_boundary_from_weights(perceptron.w_trace_[min_idx+1], axs[1])\n",
    "axs[1].arrow(0, 0, *(perceptron.w_trace_[min_idx+1]*20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Perceptron\n",
    "\n",
    "You might recognise as one improvement strategy to increase the flexibility of the model by allowing the decision boundary not to pass through the origin. Technically, this is done by introducing a **bias weight** $w_0$ and to modify the prediction function to\n",
    "$$\n",
    "y(\\boldsymbol{x}, \\boldsymbol{w}) = f(w_0 + w_1x_1 + w_2x_2)\n",
    "$$\n",
    "where $f$ denotes again our step activation function.\n",
    "\n",
    "Instead of modifying our Perceptron implementation, we can achieve this with the simple input transformation of adding a constant 1-term to the input vector:\n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\boldsymbol{x})=(1, x_1, x_2, \\dots, x_p) .\n",
    "$$\n",
    "\n",
    "As we have learned previously, we can combine this input transformation with the Perceptron into a pipeline that implements the desired overal model. For that, we could implement the transformation in the transformer framework (with methods `fit` and `transform`) or simply re-use the `PolynomialFeatures` transformer where we set the order to $1$, observing that the order $0$ feature is exactly the constant $1$ feature and the order $1$ features are our original inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "add_constant = PolynomialFeatures(1)\n",
    "add_constant.fit_transform(x_train)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task F: Build Pipeline implementing Perceptron with Intercept\n",
    "\n",
    "**Complete the line below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "perceptron_with_intercept = # YOUR CODE HERE\n",
    "perceptron_with_intercept.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_part = perceptron_with_intercept.steps[1][1]\n",
    "perceptron_part.w_, len(perceptron_part.w_trace_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_loss(perceptron_with_intercept.predict(x_train), y_train), zero_one_loss(perceptron_with_intercept.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_from_weights_with_intercept(w, ax=None):\n",
    "    slope = -w[1]/w[2]\n",
    "    intercept = -w[0]/w[2]\n",
    "    plot_line(slope, intercept, ax, shape='--', color='red')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "scatter_data_by_target_value(x_train, y_train)\n",
    "plot_decision_boundary_from_weights_with_intercept(perceptron_part.w_)\n",
    "plt.arrow(0, 0, *perceptron_part.w_[1:]*5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = perceptron_with_intercept.steps[1][1]\n",
    "test_errors = np.array([zero_one_loss((add_constant.transform(x_test).dot(w)>=0), y_test) for w in model.w_trace_])\n",
    "train_errors = np.array([zero_one_loss((add_constant.transform(x_train).dot(w)>=0), y_train) for w in model.w_trace_])\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n",
    "axs[1].plot(train_errors, label='train 0/1-error')\n",
    "axs[1].plot(test_errors, label='test 0/1-error')\n",
    "axs[1].set_xlabel(r'$\\tau$')\n",
    "axs[1].legend()\n",
    "axs[0].plot(model.w_trace_[:, 0], label='$w_0$')\n",
    "axs[0].plot(model.w_trace_[:, 1], label='$w_1$')\n",
    "axs[0].plot(model.w_trace_[:, 2], label='$w_2$')\n",
    "axs[0].set_xlabel(r'$\\tau$')\n",
    "axs[0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task G: Discussion\n",
    "\n",
    "Even with the improvement of adding a bias term, the Perceptron learning seems to be not fully satisfactory. \n",
    "\n",
    "**Make a list of problems that you see even with the improved version and give potential stragegies to address those**.\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
