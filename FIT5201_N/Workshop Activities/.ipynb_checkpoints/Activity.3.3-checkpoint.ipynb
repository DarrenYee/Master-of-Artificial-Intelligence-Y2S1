{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3.3 Bayesian Classifier\n",
    "(last modified 31 Aug 2023)\n",
    "\n",
    "### Learning Outcomes:\n",
    "\n",
    "In this activity you learn how to:\n",
    "\n",
    "- estimate mean and covariance parameters of a multi-variate normal distribution efficiently using `numpy`\n",
    "- implement different variants of the Bayesian classifier with multi-variate normal distributions as class conditional distributions\n",
    "- assess the appropriateness of different Bayesian classifier variants\n",
    "- generate artifical input data for specific classes\n",
    "- describe the advantages and disadvantages of the Bayesian classifier relative to logistic regression\n",
    "\n",
    "Prequisites:\n",
    "\n",
    "- Basics of `numpy`\n",
    "- Bayesian Classifer (Lecture 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: The Bayesian Classifier\n",
    "\n",
    "Named somewhat misleadingly, the Bayesian classifier is not a Bayesian method in the sense of defining a prior distribution on its model parameters. Instead, it gets its name because its modelled conditional class probabilities are computed via Bayes rule,\n",
    "\\begin{equation*}\n",
    "p(c_k | \\boldsymbol{x}) = \\frac{p(\\boldsymbol{x} | c_k)p(c_k)}{\\sum_{l =1}^K p(\\boldsymbol{x} | c_l)p(c_l)} \\enspace ,\n",
    "\\end{equation*}\n",
    "where the $p(\\boldsymbol{x} | c_k)$ and $p(c_k)$ are the explicitly modelled class conditional (input) distributions and the class priors, respectively.\n",
    "Specifically, we are interested in variants of this approach where the class conditionals are modelled as **multivariate normal distributions**, i.e., as following the densities\n",
    "\\begin{equation*}\n",
    "p(\\boldsymbol{x} | c_k) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma_k|}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_k)^T\\Sigma^{-1}_k(\\boldsymbol{x} - \\boldsymbol{\\mu}_k) \\right) \n",
    "\\end{equation*}\n",
    "with the **mean vectors** $\\boldsymbol{\\mu}_k \\in \\R^p$ and the **covariance matrices** $\\boldsymbol{\\Sigma}_k \\in \\R^{p \\times p}$ for each $k \\in \\{1, \\dots, K\\}$ being learnable parameters of the model. Here, the symbol $|\\boldsymbol{A}|$ is used to denote the determinant of a matrix $\\boldsymbol{A}$.\n",
    "\n",
    "Let us illustrate this distribution with two example sets of parameters that will also form the ground truth of a synthetic data generator used later in this activity. Note that the density function can be imported from `scipy` as the method `pdf` from the class `scipy.stats.multivariate_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mu0 = np.array([3.5, 2.5])\n",
    "mu1 = np.array([0.0, 6.0])\n",
    "cov0 = np.array([[4, 0.6], [0.6, 0.3]])\n",
    "cov1 = np.array([[0.4, -0.5], [-0.5, 2]])\n",
    "\n",
    "_, axs = plt.subplots(2, 2, figsize=(5.5, 5.5), sharex=True, sharey=True, tight_layout=True)\n",
    "xx1, xx2 = np.meshgrid(np.linspace(-2, 6, 100), np.linspace(1, 9, 100))\n",
    "xx = np.column_stack((xx1.ravel(), xx2.ravel()))\n",
    "pp1 = multivariate_normal.pdf(xx, mu0, np.diag(np.diag(cov0))).reshape(100, 100)\n",
    "pp2 = multivariate_normal.pdf(xx, mu1, np.diag(np.diag(cov1))).reshape(100, 100)\n",
    "pp3 = multivariate_normal.pdf(xx, mu0, cov0).reshape(100, 100)\n",
    "pp4 = multivariate_normal.pdf(xx, mu1, cov1).reshape(100, 100)\n",
    "axs[0, 0].contourf(xx1, xx2, pp1, levels=50) \n",
    "axs[0, 1].contourf(xx1, xx2, pp2, levels=50) \n",
    "axs[1, 0].contourf(xx1, xx2, pp3, levels=50) \n",
    "cp = axs[1, 1].contourf(xx1, xx2, pp4, levels=50)\n",
    "axs[0, 0].text(0.02, 0.98, f'$\\mu=({mu0[0]}, {mu0[1]})$', color='white', transform=axs[0, 0].transAxes, verticalalignment='top')\n",
    "axs[0, 0].text(0.02, 0.90, f'$\\Sigma=({cov0[0][0]}, 0; 0, {cov0[1][1]})$', color='white', transform=axs[0, 0].transAxes, verticalalignment='top')\n",
    "axs[1, 0].text(0.02, 0.98, f'$\\mu=({mu0[0]}, {mu0[1]})$', color='white', transform=axs[1, 0].transAxes, verticalalignment='top')\n",
    "axs[1, 0].text(0.02, 0.90, f'$\\Sigma=({cov0[0][0]}, {cov0[0][1]}; {cov0[1][0]}, {cov0[1][1]})$', color='white', transform=axs[1, 0].transAxes, verticalalignment='top')\n",
    "axs[0, 1].text(0.02, 0.15, f'$\\mu=({mu1[0]}, {mu1[1]})$', color='white', transform=axs[0, 1].transAxes, verticalalignment='top')\n",
    "axs[0, 1].text(0.02, 0.08, f'$\\Sigma=({cov1[0][0]}, 0; 0, {cov1[1][1]})$', color='white', transform=axs[0, 1].transAxes, verticalalignment='top')\n",
    "axs[1, 1].text(0.02, 0.15, f'$\\mu=({mu1[0]}, {mu1[1]})$', color='white', transform=axs[1, 1].transAxes, verticalalignment='top')\n",
    "axs[1, 1].text(0.02, 0.08, f'$\\Sigma=({cov1[0][0]}, {cov1[0][1]}; {cov1[1][0]}, {cov1[1][1]})$', color='white', transform=axs[1, 1].transAxes, verticalalignment='top')\n",
    "axs[0, 0].set_ylabel('$x_2$')\n",
    "axs[1, 0].set_ylabel('$x_2$')\n",
    "axs[1, 0].set_xlabel('$x_1$')\n",
    "axs[1, 1].set_xlabel('$x_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now are are going to generate a training dataset using the first set of covariance matrices (the simple unit case with spherical densities), which we will use as our first test case for the Bayes classifier variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activity3 import make_gaussian_mixture_data, scatter_data_by_target_value\n",
    "\n",
    "p0 = 0.60; p1 = 1 - p0\n",
    "class_probs = [p0, p1] \n",
    "mu = np.array([mu0, mu1])\n",
    "\n",
    "n = 20\n",
    "x_train, y_train = make_gaussian_mixture_data(n, mu, class_probs=class_probs, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "scatter_data_by_target_value(x_train, y_train)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVN Parameter Estimation\n",
    "\n",
    "To implement the Bayes classifier we need to be able to compute mean vectors and covariance matrices as well as class probabilities. For large datasets (and potentially many classes) it is important for computational efficiency to give vectorised implementations, i.e., those that avoid explicit loops over the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by exploring how we can efficiently estimate the class prior probabilities. At this point, it should be relatively easy for you to do this with a loop over the classes and then a numpy selection for each class. Another nice way which avoids explicitly looping over the classes is to use [`numpy.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) with the option `return_counts=True`.\n",
    "\n",
    "#### Task A: Estimate Class Prior Probabilities\n",
    "\n",
    "**Estimate the class probabilities and store them in an array under the name `class_probs_hat`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class_probs_hat = # YOUR CODE HERE\n",
    "\n",
    "(class_probs - class_probs_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us turn to the conditional means. We combine this exercise with a plotting function that allows us to show the estimated means relative to the true means. Again, at this point it should be quite straightforward for you to compute the mean of an array (using the `mean` method). Here, our task is only a little bit more complicated as we have to compute means for all input columns for all classes. Below we implement this only for two classes but it would be easy to extend this to more using a loop. \n",
    "\n",
    "#### Task B: Compute Data Mean per Class\n",
    "\n",
    "**Finish the implementation of the plotting function below by computing the input mean vectors per class.**\n",
    "\n",
    "Hints: Rememember how we\n",
    "- used component-wise comparisons to generate Boolean arrays that are True for each index that corresponds to a data point of a specific class\n",
    "- used fancy indexing to select a subset of an array based on a Boolean (or integer) array\n",
    "- computed the vector of all column means of a matrix by calling the `mean` axis along the right axis (using the `axis` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from activity3 import make_gaussian_mixture_data, scatter_data_by_target_value\n",
    "\n",
    "def scatter_data_with_means(x, y, mu):\n",
    "    mu0_hat = # YOUR CODE HERE TO COMPUTE THE MEAN VECTOR OF CLASS 0\n",
    "    mu1_hat = # YOUR CODE HERE TO COMPUTE THE MEAN VECTOR OF CLASS 1\n",
    "    mu_hat = np.array([mu0_hat, mu1_hat])\n",
    "    scs = scatter_data_by_target_value(x, y)\n",
    "    for c in range(2):\n",
    "        plt.scatter(mu_hat[c][0], mu_hat[c][1], fc=scs[c].get_facecolor(), marker='s', edgecolors='red', s=[60])\n",
    "        plt.scatter(mu[c][0], mu[c][1], marker='*', fc=scs[c].get_facecolor(), edgecolors='red', s=[60])\n",
    "    plt.scatter([], [], marker='*', fc='white', edgecolors='black', s=[60], label=r'$\\mu_c$')\n",
    "    plt.scatter([], [], marker='s', fc='white', edgecolors='black', s=[60], label=r'$\\hat{\\mu}_c$')\n",
    "\n",
    "p0 = 0.60; p1 = 1 - p0\n",
    "class_probs = [p0, p1] \n",
    "mu0 = np.array([3.5, 2.5])\n",
    "mu1 = np.array([0.0, 6.0])\n",
    "mu = np.array([mu0, mu1])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "scatter_data_with_means(x_train, y_train, mu)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to the (co-)variance estimation, it is again quite simple to estimate individual covariances via the definition of the maximum likelihood estimator\n",
    "\\begin{equation*}\n",
    "\\sigma^{(k)}_{ij} = \\frac{1}{N_k} \\sum_{n \\in D_k} \\left(x_{n,i}-\\hat{\\mu}^{(k)}_{n,i}\\right)\\left(x_{n,j}-\\hat{\\mu}^{(k)}_{n,j}\\right)\n",
    "\\end{equation*}\n",
    "as done in the next cell. Note that this definition yields the variances as special case for $i=j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x_train[y_train == 0]\n",
    "((a[:, 0]-a[:, 0].mean())*(a[:, 1]-a[:, 1].mean())).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this method is not ideal because it would require us to explicitly loop over all combinations of input variables, which can in general be a large number. Instead we are looking for an implementation that is also vectorised in terms of $d$. Only for the variances (diagonal elements of the covariance matrix), this can be achieved via the `var` method of arrays as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this does not help us for the variants where we have to estimate full covariance matrices. For that, we can use [`numpy.cov`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) which estimates covariance matrices from data samples. The only two pitfalls here are that:\n",
    "\n",
    "1. the function assumes per default that the data points are given as the columns of the input matrix (as opposed to our convention to store them as rows) and\n",
    "2. that it computes not the maximum likelihood estimate but a slighlty modified version that corrects a finite sample bias of the MLE. \n",
    "\n",
    "Both can be changed via the options `rowvar=False` and `bias=True`, respectively.\n",
    "\n",
    "#### Task C: Find Covariance Estimates \n",
    "\n",
    "**Use `numpy.cov` as described above to compute the MLE of the covariance matrix for class 0.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_hat = # YOUR CODE HERE\n",
    "cov_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Bayes Classifier\n",
    "\n",
    "We are now ready to put everything together and implement the Bayes classifer. The implementation below stores during `fit`:\n",
    "\n",
    "- the class priors in `self.class_priors_` as array with shape `(k,)`\n",
    "- the conditional means in `self.cond_means_` as array with shape `(k, p)`\n",
    "- the conditional covariance matrices in `self.cond_cov_` as array with shape `(k, p, p)`\n",
    "\n",
    "The implementationa also correctly handles the case of a shared covariance matrix (option `shared_cov`) and whether to use full covariance matrices or Naive Bayes (`cond_ind`).\n",
    "\n",
    "#### Task D: Implement `predict_proba`\n",
    "\n",
    "**Finish the method code below by computing the conditional probabilities of all `m` test data points for each class and the vector of marginal probalities of all input data points (i.e., the denominator in Bayes rule).**\n",
    "\n",
    "Hints: \n",
    "\n",
    "- After the loop, the array `cond_probs` should contain the conditional probabilities (densities) $p(x_i | c)$ in row $i$ and column $c$. Luckily, `scipy.multivariate_normal.pdf` can provide vectorised densities for an array of test points `x`.\n",
    "- If the columns of `cond_probs` contain the probabilities of all test points for a given class, then computing the marginal probabilities $p(x)$ comes down to form an appropriate linear combination of those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class BayesianClassifier:\n",
    "\n",
    "    def __init__(self, shared_cov=True, cond_ind=True):\n",
    "        self.shared_cov=shared_cov\n",
    "        self.cond_ind=cond_ind\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        self.n_ , self.p_ = x.shape\n",
    "        self.k_ = len(self.classes_)\n",
    "        self.cond_means_ = np.zeros(shape=(self.k_, self.p_))\n",
    "        self.cond_covs_ = np.zeros(shape=(self.k_, self.p_, self.p_))\n",
    "        \n",
    "        self.class_priors_ = class_counts/len(y)\n",
    "        for c in range(self.k_):\n",
    "            c_rows = y==c\n",
    "            self.cond_means_[c, :] = x[c_rows].mean(axis=0)\n",
    "            if self.cond_ind:\n",
    "                np.fill_diagonal(self.cond_covs_[c, :, :], x[c_rows].var(axis=0))\n",
    "            else:\n",
    "                self.cond_covs_[c, :, :] = np.cov(x[c_rows].T, bias=True)\n",
    "\n",
    "        if self.shared_cov:\n",
    "            shared_cov = np.moveaxis(self.cond_covs_, 0, -1).dot(self.class_priors_)\n",
    "            self.cond_covs_[:] = shared_cov\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        m, _ = x.shape\n",
    "        cond_probs = np.zeros(shape=(m, self.k_))\n",
    "        for c in range(self.k_):\n",
    "            # find p(x | c_k)\n",
    "            # singular covariance matrices could happen (e.g., through inaccurate estimation)\n",
    "            cond_probs[:, c] = multivariate_normal.pdf(x, \n",
    "                                                       # YOUR CODE HERE\n",
    "                                                       # YOUR CODE HERE\n",
    "                                                       allow_singular=True)\n",
    "        # find marginal probabilities p(x) by summing all the conditionals weighted by the priors\n",
    "        marginal_probs = # YOUR CODE HERE\n",
    "\n",
    "        # find probability vector (p(c1 | x), ..., p(ck | x)) via p(ci | x)=p(x | ci) / p(x)\n",
    "        # however, p(x) might have been rounded to 0\n",
    "        # thus, compute via case distinction\n",
    "        probs = np.divide((cond_probs*self.class_priors_).T,\n",
    "                          marginal_probs,\n",
    "                          where=marginal_probs>0, out=np.zeros(shape=(self.k_, m))).T\n",
    "        return probs\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.predict_proba(x), axis=1)\n",
    "\n",
    "    def decision_function(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        if self.k_ == 2:\n",
    "            return np.log(probs[:, 1]/probs[:, 0])\n",
    "        else:\n",
    "            res = np.zeros(len(x), self.k_)\n",
    "            for c in range(self.k_):\n",
    "                res[:, c]=np.log(probs[:, c]/(1-probs[:, c]))\n",
    "            return res\n",
    "        \n",
    "    def generate(self, n, c, random_state=None):\n",
    "        return multivariate_normal.rvs(self.cond_means_[c], self.cond_covs_[c], size=n, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cab now run the different variants and see how well they work.\n",
    "\n",
    "#### Task E: Formulate your Expectations\n",
    "\n",
    "Before looking at the results below, discuss your expectations by answering the following questions.\n",
    "\n",
    "**Which of the four model variants do you expect to perform best on the example setting where both classes have unit covariances? Why? How many parameters are estimated by the different variants?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss, log_loss\n",
    "\n",
    "def plot_model_performances(models, model_names, x_train, y_train, x_test, y_test):\n",
    "    train_01_losses = []\n",
    "    train_log_losses = []\n",
    "    test_01_losses = []\n",
    "    test_log_losses = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        train_01_losses.append(zero_one_loss(y_train, model.predict(x_train)))\n",
    "        train_log_losses.append(log_loss(y_train, model.predict_proba(x_train)))\n",
    "        test_01_losses.append(zero_one_loss(y_test, model.predict(x_test)))\n",
    "        test_log_losses.append(log_loss(y_test, model.predict_proba(x_test)))\n",
    "\n",
    "    xx = np.arange(len(models))\n",
    "    bar_width = 1/(len(models)+1)\n",
    "    group_width = len(models)*bar_width\n",
    "    plt.bar(xx-group_width/2, train_log_losses, width=bar_width, label='log loss (train)')\n",
    "    plt.bar(xx-group_width/2 + bar_width, test_log_losses, width=bar_width, label='log loss (test)')\n",
    "    plt.bar(xx-group_width/2 + 2*bar_width, train_01_losses, width=bar_width, label='0/1 loss (train)')\n",
    "    plt.bar(xx-group_width/2 +3*bar_width, test_01_losses, width=bar_width, label='0/1 loss (test)')\n",
    "    plt.xticks(xx, model_names)\n",
    "\n",
    "x_test, y_test = make_gaussian_mixture_data(10000, mu, class_probs=class_probs, random_state=1)\n",
    "\n",
    "nb_shared = BayesianClassifier(shared_cov=True, cond_ind=True).fit(x_train, y_train)\n",
    "nb = BayesianClassifier(shared_cov=False, cond_ind=True).fit(x_train, y_train)\n",
    "bc_shared = BayesianClassifier(shared_cov=True, cond_ind=False).fit(x_train, y_train)\n",
    "bc = BayesianClassifier(shared_cov=False, cond_ind=False).fit(x_train, y_train)\n",
    "\n",
    "models = [nb_shared, nb, bc_shared, bc]\n",
    "model_names = ['NB (shared)', 'NB', 'BC (shared)', 'BC']\n",
    "\n",
    "plt.subplots(1, 1, figsize=(5, 3))\n",
    "plot_model_performances(models, model_names, x_train, y_train, x_test, y_test)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs_and_log_odds(models, model_names, x, y, rel_margin=0.1):\n",
    "    _ , axs = plt.subplots(2, len(models), figsize=(3*len(model_names), 7), sharex=True, sharey=True, tight_layout=True)\n",
    "    \n",
    "    a1, a2 = x.min(axis=0)\n",
    "    b1, b2 = x.max(axis=0)\n",
    "    x_margin = rel_margin*(b1-a1)\n",
    "    y_margin = rel_margin*(b2-a2)\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(a1-x_margin, b1+x_margin, 100), np.linspace(a2-y_margin, b2+y_margin, 100))\n",
    "    xx = np.column_stack((xx1.ravel(), xx2.ravel()))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        pp = model.predict_proba(xx)\n",
    "        \n",
    "        axs[0][i].contourf(xx1, xx2, pp[:, 1].reshape(100, 100), levels=50)\n",
    "        scatter_data_by_target_value(x, y, ax=axs[0][i])\n",
    "        axs[0][i].set_title(model_names[i])\n",
    "\n",
    "        aa = model.decision_function(xx)\n",
    "        axs[1][i].contourf(xx1, xx2, aa.reshape(100, 100), levels=50)\n",
    "        axs[1][i].contour(xx1, xx2, aa.reshape(100, 100), levels=[0.0], colors='black')\n",
    "        scatter_data_by_target_value(x, y, ax=axs[1][i])\n",
    "\n",
    "        if i>0: \n",
    "            axs[0][i].set_ylabel(None)\n",
    "            axs[1][i].set_ylabel(None)\n",
    "        axs[0][i].set_xlabel(None)\n",
    "        axs[0][i].set_xlim((a1-x_margin, b1+x_margin))\n",
    "        axs[0][i].set_ylim((a2-y_margin, b2+y_margin))\n",
    "\n",
    "plot_probs_and_log_odds(models, model_names, x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complex Example\n",
    "\n",
    "We now turn to the more complicated inputs with non-unit covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov1 = np.array([[4, 0.6], [0.6, 0.3]])\n",
    "# cov2 = np.array([[0.4, -0.5], [-0.5, 2]])\n",
    "\n",
    "n2 = 50\n",
    "x2_train, y2_train = make_gaussian_mixture_data(n2, mu, covs=[cov0, cov1], class_probs=class_probs, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "scatter_data_with_means(x2_train, y2_train, mu)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task F: Formulate your Expectations\n",
    "\n",
    "Before looking at the results below, again discuss your expectations.\n",
    "\n",
    "**Which of the four model variants do you expect to perform best in the more complex setting? Why? How do you expect the variant with full but shared covariance to work?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_test, y2_test = make_gaussian_mixture_data(10000, mu, covs=[cov0, cov1], class_probs=class_probs, random_state=1)\n",
    "nbs2 = BayesianClassifier(shared_cov=True, cond_ind=True).fit(x2_train, y2_train)\n",
    "nb2 = BayesianClassifier(shared_cov=False, cond_ind=True).fit(x2_train, y2_train)\n",
    "bcs2 = BayesianClassifier(shared_cov=True, cond_ind=False).fit(x2_train, y2_train)\n",
    "bc2 = BayesianClassifier(shared_cov=False, cond_ind=False).fit(x2_train, y2_train)\n",
    "models2 = [nbs2, nb2, bcs2, bc2]\n",
    "model_names = ['NB (shared)', 'NB', 'BC (shared)', 'BC']\n",
    "\n",
    "plt.subplots(1, 1, figsize=(5, 3))\n",
    "plot_model_performances(models2, model_names, x2_train, y2_train, x2_test, y2_test)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probs_and_log_odds(models2, model_names, x2_train, y2_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Artificial Data\n",
    "\n",
    "Finally, we can test the generative capabilities of the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = bc2.generate(5, 1, 0)\n",
    "\n",
    "plt.subplots(1, 1, figsize=(4, 4))\n",
    "scatter_data_by_target_value(x2_train, y2_train)\n",
    "plt.scatter(generated[:, 0], generated[:, 1], ec='black', label='generated for $c=1$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the above is a toy example, but imagine if we learned a generative model for images of different classes of animals based on a large-scale image dataset. One can then generate new unseen images of the same classes of animals. \n",
    "\n",
    "Check [this](https://stablediffusionweb.com/) out to see examples from state-of-the-art text-to-image generative models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "#### Task: List the advantages and disadvantages of the Bayes classifier relative to logistic regression\n",
    "\n",
    "Hint: For the disadvantages, think about how the number of parameters grows for high-dimensional input variables\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
