{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actrivity 2.2. Regularisation\n",
    "Last modified (21 Feb 2023)\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "In this activity we learn how to\n",
    "\n",
    "- running and evaluating ridge and lasso regression using `scikit-learn`\n",
    "- investigate the fitted weight vectors of linear models\n",
    "- to use lasso regression for feature selection\n",
    "- to relate the regularisation parameter to model complexity, overfitting, and underfitting\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Lecture 3\n",
    "- Activity 2.1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the lecture we have introduced the idea of regularised least squares regression. That means that instead of fitting the weight by minimising the mean squared error\n",
    "\\begin{equation*}\n",
    "E(\\mathbf{w}) = \\frac{1}{2N} \\sum_{n=1}^N (t_i - y(\\mathbf{x}_i, \\mathbf{w}))^2\n",
    "\\end{equation*}\n",
    "we can minimise the weighted sum of the error and (squared) norm of the weight vector. In particular, we looked at the two variants\n",
    "\\begin{align*}\n",
    "E^{(L_2)}_\\lambda(\\mathbf{w}) &= E(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_2^2\\\\\n",
    "E^{(L_1)}_\\lambda(\\mathbf{w}) &= E(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_1\n",
    "\\end{align*}\n",
    "where the first is known as **ridge regression** and the second is known as **lasso regression** where $\\lambda$ is known as the **regularisation parameter**. Here, we will test and evaluate these methods on a real-world regression task.\n",
    "\n",
    "Note that we will not implement the fitting algorithms here but instead use the implementation from `scikit-learn`. In Assignment 1 you will work on fitting a ridge regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this activity we work with another popular real-world dataset that is often used as an example regression problem: the [diabetes](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) dataset, which is also available through `scikit-learn`. The task associated with this dataset is to determine the progression of the diabetes disease (measured quantitively as a real number) in a patient based on observed blood markers and other physiological measurements.\n",
    "\n",
    "**Note: When we load the dataset with the default parameters of the `load_diabetes` function, all input variables are standardised, which is very important when applying regularised regression methods. In the future, when working with other datasets you have to transform the data yourself to be standardised (in the next activity we will look at transformations).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], (442, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "diabetes.feature_names, diabetes.data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the data is indeed already normalised such that columns all have a standard deviation of $\\sqrt{N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.data.std(axis=0)*len(diabetes.data)**0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the target variable is not normalised, but this is not critical for our regularisation, as it affects all weights equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1618.953095192813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.target.std()*len(diabetes.data)**0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and Basic Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task A: Fit models\n",
    "\n",
    "**Complete the code to perform the following tasks:**\n",
    "\n",
    "- **Split the diabetes data and target into train and test portion using 80% of the data as training.** Hint: we have implemented a function for data splitting in Activity 1.1. Conveniently, the same function is available as `sklearn.model_selection.train_test_split`. Use a fixed random seed so that your results are deterministic and, hence, your analysis will be consistent with the generated images.\n",
    "\n",
    "- **Fit an ordinary least squares (linear) model, a ridge regression, and a lasso regression model to the training data. For the two regularised variants use a regularisation parameter value of $0.1$.** Note: We are not implementing fitting algorithms here, but instead simple use the estimators provided in `sklearn.linear_model`, namely `LinearRegression`, [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), and [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, train_size=0.8, random_state=1)\n",
    "\n",
    "linear = LinearRegression().fit(x_train, y_train)\n",
    "ridge = Ridge(alpha = 0.1).fit(x_train, y_train)\n",
    "lasso = Lasso(alpha = 0.1).fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B: Formulate and test your expectations\n",
    "\n",
    "**Briefly formulate your expectation about the following questions and then test your hypotheses by computing the corresponding quantities. How will the three models rank in terms of:** \n",
    "\n",
    "- train mean squared error\n",
    "- test mean squared error\n",
    "- Euclidean (L2) norm of fitted coefficients \n",
    "- Manhatten (L1) norm of fitted coefficients\n",
    "\n",
    "Hints:\n",
    "\n",
    "- You can implement the mean squared error computation from hand (using numpy or plain Python) or simply import the corresponding metric from [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "- Similar, you can compute the coefficient norms with your own plain Python or numpy function or use [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) (the ord parameter can be used to determine what norm is computed).\n",
    "- Remember that the coeffficents of the fitted linear models can be accessed via `coef_` (where the trailing underscore indicated that the attribute is only available after fitting).\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso  53.24506757085851 54.70449002870804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.662572596580894, 55.0659448252393)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# YOUR CODE HERE FOR DETERMINING TRAIN ERRORS\n",
    "import math\n",
    "def error_rate(y, y_hat):\n",
    "    error = mean_squared_error(y,y_hat)\n",
    "    return math.sqrt(error)\n",
    "    \n",
    "y_hat_test = linear.predict(x_test)\n",
    "y_hat_train = linear.predict(x_train)\n",
    "print (\"Linear \",error_rate(y_train, y_hat_train), error_rate(y_test, y_hat_test))\n",
    "\n",
    "y_hat_test = ridge.predict(x_test)\n",
    "y_hat_train = ridge.predict(x_train)\n",
    "print (\"Ridge \",error_rate(y_train, y_hat_train), error_rate(y_test, y_hat_test))\n",
    "\n",
    "y_hat_test = lasso.predict(x_test)\n",
    "y_hat_train = lasso.predict(x_train)\n",
    "print (\"Lasso \",error_rate(y_train, y_hat_train), error_rate(y_test, y_hat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR DETERMINING TEST ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "# YOUR CODE HERE FOR DETERMINING L2 NORM OF WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR DETERMINING L1 NORM OF WEIGHTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso for Feature Selection\n",
    "\n",
    "As mentioned in the lecture, the L1-regularisation employed by lasso will, given suitable choices of $\\lambda$, lead to sparse weight vectors, i.e., weight vectors where a subset of the weights are $0$. This is in contrast to the L2-regularisation employed by ridge regression.\n",
    "\n",
    "This can be interpreted as a variable/feature selection procedure where the lasso selects (deems relevant) variables that have a non-zero weight and deselects (deems irrelevant) those that have a zero weight."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task C: Determine Lasso Feature Selection\n",
    "\n",
    "**Compile lists of the names of the features that lasso has selected as relevant and irrelevant, respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR COMPILING LIST OF SELECTED VARIABLE NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR COMPILING LIST OF NOT SELECTED VARIABLE NAMES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Regularisation Parameter Value\n",
    "\n",
    "After having investigated the regularised models for a single value of the regularisation parameter (0.1), let us finally see how the model fits will evolve when changing that value. In fact, when using these models, you would always check a wide range of different $\\lambda$-values as it is a priori unclear even what order of magnitude will lead to a good fit.\n",
    "\n",
    "Therefore one typically tries an exponentially spaced grid of candidate values which can be generated with [`numpy.logspace`](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-4, 2.0, 200)\n",
    "ridges = [Ridge(alpha=alpha) for alpha in alphas]\n",
    "lassos = [Lasso(alpha=alpha) for alpha in alphas]\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    ridges[i].fit(x_train, y_train)\n",
    "    lassos[i].fit(x_train, y_train)\n",
    "\n",
    "ridge_test_errors = [mean_squared_error(y_test, ridge.predict(x_test)) for ridge in ridges]\n",
    "lasso_test_errors = [mean_squared_error(y_test, lasso.predict(x_test)) for lasso in lassos]\n",
    "ridge_train_errors = [mean_squared_error(y_train, ridge.predict(x_train)) for ridge in ridges]\n",
    "lasso_train_errors = [mean_squared_error(y_train, lasso.predict(x_train)) for lasso in lassos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the corresponding train and test errors as well as weight evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(12, 4), tight_layout=True)\n",
    "axs[0].axhline(mean_squared_error(linear.predict(x_test), y_test), label='least squares test', color='black', linestyle='-')\n",
    "axs[0].axhline(mean_squared_error(linear.predict(x_train), y_train), label='least squares train', color='black', linestyle='--')\n",
    "lines = axs[0].plot(alphas, ridge_test_errors, label='ridge test')\n",
    "axs[0].plot(alphas, ridge_train_errors, linestyle='--', color=lines[0].get_color(), label='ridge train')\n",
    "lines = axs[0].plot(alphas, lasso_test_errors, label='lasso test')\n",
    "axs[0].plot(alphas, lasso_train_errors, linestyle='--', color=lines[0].get_color(), label='lasso train')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel('mean squared error')\n",
    "for j in range(len(linear.coef_)):\n",
    "    axs[1].plot(alphas, [ridge.coef_[j] for ridge in ridges], label=diabetes.feature_names[j])\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_ylabel('ridge weights')\n",
    "for j in range(len(diabetes.feature_names)):\n",
    "    axs[2].plot(alphas, [lasso.coef_[j] for lasso in lassos], label=diabetes.feature_names[j])\n",
    "axs[2].set_xscale('log')\n",
    "axs[2].legend()\n",
    "axs[2].set_ylabel('lasso weights')\n",
    "for j in range(3):\n",
    "    axs[j].set_xlabel('$\\lambda$')\n",
    "    axs[j].invert_xaxis()\n",
    "    axs[j].margins(x=0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "#### Task D: Analyse the figure generated above\n",
    "\n",
    "**Answer the following questions:**\n",
    "\n",
    "- **Can you relate the different $\\lambda$-values to the concept of underfitting and overfitting?**\n",
    "\n",
    "- **Based on the one train/test split that we have performed, what seems to be the best model?**\n",
    "\n",
    "- **Is there something to note about the sign of the weights and the magnitude of the weights as $\\lambda$ decreases? What does this imply when we want to interpret the weights as direction and magnitude of effect that a variable has on the target?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n",
    "### Further optional activities\n",
    "\n",
    "As final questions and suggestion for study according to your own curiosity:\n",
    "\n",
    "- What is the risk with the conclusions above? \n",
    "- How could we increase the reliability of our model assessment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
