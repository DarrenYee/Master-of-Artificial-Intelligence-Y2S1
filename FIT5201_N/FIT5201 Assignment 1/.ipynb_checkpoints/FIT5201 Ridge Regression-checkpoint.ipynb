{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3271434f",
   "metadata": {},
   "source": [
    "**Question 6.1 Derivation of Weight Update Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb43e0",
   "metadata": {},
   "source": [
    "1) Ridge Regression Function (as defined in Module 2 Chapter 4 Page 9)  \n",
    "    $E(w) = \\frac{1}{2} \\sum_{n=1}^{N} (t_n - w^T \\phi(x_n))^2 + \\frac{\\lambda}{2} w^T . w$  \n",
    "    \n",
    "  \n",
    "2) Derivation of the above function to obtain gradient (derive function wrt W)  \n",
    "    $\\nabla E(w) = \\sum_{n=1}^{N} (w^T \\phi(x_n) - t_n) \\phi(x_n) + \\lambda w$  \n",
    "    \n",
    "3) SGD weight update rule (as defined in Module 2 Chapter 2 Page 7)  \n",
    "    $w_{n} = w_{n-1} - \\eta \\nabla E(w_{n-1})$\n",
    "    \n",
    "4) Putting it all together, we can get the following formula for weight update using SGD  \n",
    "\n",
    "    $w_n = w_{n-1} - \\eta \\left[ (w_{n-1}^T \\phi(x_n) - t_n) \\phi(x_n) + \\lambda w_{n-1} \\right]$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a53375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Class Adpater from Activity 2.1 of Module 2\n",
    "class SGDRidgeRegressor:\n",
    "\n",
    "    def __init__(self, batch_size=1, eta=0.01, tau_max=1000, epsilon=0.00001, random_state=None, lam = 0.1):\n",
    "        self.eta = eta\n",
    "        self.tau_max = tau_max\n",
    "        self.epsilon = epsilon\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.lam = lam\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        RNG = np.random.default_rng(self.random_state)\n",
    "        n, p = x.shape\n",
    "        self.w_ = np.zeros(shape=(self.tau_max+1, p))\n",
    "        for tau in range(1, self.tau_max+1):\n",
    "            idx = RNG.choice(n, size=self.batch_size, replace=True)\n",
    "            grad = (x[idx].dot(self.w_[tau-1])-y[idx] + (self.lam * self.w_[tau-1]))/ self.batch_size\n",
    "            self.w_[tau] = self.w_[tau-1] - self.eta*grad\n",
    "            if np.linalg.norm(self.w_[tau]-self.w_[tau-1]) < self.epsilon:\n",
    "                break\n",
    "        self.coef_ = self.w_[tau] \n",
    "        self.w_ = self.w_[:tau+1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        return x.dot(self.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41407050",
   "metadata": {},
   "source": [
    "**Question 6.3 L2 Regularisaton training and testing errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9a9c793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(5*np.pi*x)/(1+2*x)\n",
    "\n",
    "def make_additive_noise_data(n, f, a, b, noise=0.1**0.5, random_state=None):\n",
    "    RNG = np.random.default_rng(random_state)\n",
    "    x = RNG.uniform(a, b, size=(n, 1))\n",
    "    y = f(x) + RNG.normal(0, noise, size=(n, 1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b154918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, a, b, models=[], data=None, ax=None, ax_labels=True, legend=True):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    xx = np.linspace(a, b, 200).reshape(-1, 1)\n",
    "    if len(models)==1:\n",
    "        ax.fill_between(xx.squeeze(), f(xx).squeeze(), models[0].predict(xx).squeeze(), alpha=0.3)\n",
    "        ax.plot(xx, models[0].predict(xx), label='$y$')\n",
    "    if len(models) > 1:\n",
    "        for model in models: ax.plot(xx, model.predict(xx), color='gray', alpha=0.5)\n",
    "    ax.plot(xx, f(xx), color='black', label='$f$')\n",
    "    if data is not None:\n",
    "        x, y = data\n",
    "        ax.scatter(x, y, marker='.', color='black', label='data')\n",
    "    if ax_labels:\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$t$')\n",
    "    if legend: ax.legend()\n",
    "    ax.margins(x=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5f745e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialFeatures:\n",
    "\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        output = []\n",
    "        for i in range(0, self.degree+1):\n",
    "            column = x**i\n",
    "            output.append(column)\n",
    "        return np.column_stack(output)\n",
    "\n",
    "    def fit_transform(self, x, y=None):\n",
    "        self.fit(x, y)\n",
    "        return self.transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc3eb543",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(lambda_choices)):\n\u001b[0;32m     18\u001b[0m     transformation_then_ridge \u001b[38;5;241m=\u001b[39m make_pipeline(poly, SGDRidgeRegressor(lam \u001b[38;5;241m=\u001b[39m lambda_choices[i]))\n\u001b[1;32m---> 19\u001b[0m     transformation_then_ridge\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m     21\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m transformation_then_ridge\u001b[38;5;241m.\u001b[39mpredict(x_train)\n\u001b[0;32m     22\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m transformation_then_ridge\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[107], line 18\u001b[0m, in \u001b[0;36mSGDRidgeRegressor.fit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     idx \u001b[38;5;241m=\u001b[39m RNG\u001b[38;5;241m.\u001b[39mchoice(n, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m     grad \u001b[38;5;241m=\u001b[39m (x[idx]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m-\u001b[39my[idx] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta\u001b[38;5;241m*\u001b[39mgrad\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau]\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import random\n",
    "\n",
    "poly = PolynomialFeatures(5)\n",
    "lambda_choices = np.geomspace(10**-10,0.1, 101, endpoint=True)\n",
    "models = []\n",
    "reps = 10\n",
    "train_mse = [[] for i in range (len(lambda_choices))]\n",
    "test_mse = [[] for i in range (len(lambda_choices))]\n",
    "\n",
    "for rep in range (10):\n",
    "    x_train, y_train = make_additive_noise_data(20, f, -0.3, 0.3, random_state=2)\n",
    "    x_test, y_test = make_additive_noise_data(20, f, -0.3, 0.3, random_state=1)\n",
    "    \n",
    "    for i in range (len(lambda_choices)):\n",
    "        \n",
    "        transformation_then_ridge = make_pipeline(poly, SGDRidgeRegressor(lam = lambda_choices[i]))\n",
    "        transformation_then_ridge.fit(x_train, y_train)\n",
    "        \n",
    "        y_train_pred = transformation_then_ridge.predict(x_train)\n",
    "        y_test_pred = transformation_then_ridge.predict(x_test)\n",
    "        \n",
    "        train_mse[i].append(mean_squared_error(y_train, y_train_pred))\n",
    "        test_mse[i].append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "train_mse_mean = np.array([np.mean(mse) for mse in train_mse])\n",
    "test_mse_mean = np.array([np.mean(mse) for mse in test_mse])    \n",
    "\n",
    "# Plot the mean squared errors on log-log scale plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_choices, train_mse_mean, label='Training MSE', color='blue')\n",
    "plt.plot(lambda_choices, test_mse_mean, label='Testing MSE', color='red')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('log lambda')\n",
    "plt.ylabel('log mean squared error')\n",
    "plt.title('Mean Squared Errors vs. Log Lambda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "06f51007",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(lambda_choices)):\n\u001b[0;32m     21\u001b[0m     transformation_then_ridge \u001b[38;5;241m=\u001b[39m make_pipeline(poly, SGDRidgeRegressor(lam \u001b[38;5;241m=\u001b[39m lambda_choices[i]))\n\u001b[1;32m---> 22\u001b[0m     transformation_then_ridge\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m     24\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m transformation_then_ridge\u001b[38;5;241m.\u001b[39mpredict(x_train)\n\u001b[0;32m     25\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m transformation_then_ridge\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[107], line 18\u001b[0m, in \u001b[0;36mSGDRidgeRegressor.fit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_max\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     idx \u001b[38;5;241m=\u001b[39m RNG\u001b[38;5;241m.\u001b[39mchoice(n, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m     grad \u001b[38;5;241m=\u001b[39m (x[idx]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m-\u001b[39my[idx] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta\u001b[38;5;241m*\u001b[39mgrad\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau]\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_[tau\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "poly = PolynomialFeatures(5)\n",
    "lambda_choices = np.geomspace(10**-10,0.2, 100, endpoint=True)\n",
    "models = []\n",
    "reps = 10\n",
    "train_mse = [[] for i in range (len(lambda_choices))]\n",
    "test_mse = [[] for i in range (len(lambda_choices))]\n",
    "\n",
    "for rep in range (10):\n",
    "    \n",
    "    x_data, y_data = make_additive_noise_data(40, f, -0.3, 0.3, random_state=1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, shuffle=True ,random_state=1)\n",
    "    \n",
    "    for i in range (len(lambda_choices)):\n",
    "        \n",
    "        transformation_then_ridge = make_pipeline(poly, SGDRidgeRegressor(lam = lambda_choices[i]))\n",
    "        transformation_then_ridge.fit(x_train, y_train)\n",
    "        \n",
    "        y_train_pred = transformation_then_ridge.predict(x_train)\n",
    "        y_test_pred = transformation_then_ridge.predict(x_test)\n",
    "        \n",
    "        train_mse[i].append(mean_squared_error(y_train, y_train_pred))\n",
    "        test_mse[i].append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "train_mse_mean = np.array([np.mean(mse) for mse in train_mse])\n",
    "test_mse_mean = np.array([np.mean(mse) for mse in test_mse])    \n",
    "\n",
    "# Plot the mean squared errors on log-log scale plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_choices, train_mse_mean, label='Training MSE', color='blue')\n",
    "plt.plot(lambda_choices, test_mse_mean, label='Testing MSE', color='red')\n",
    "plt.xlabel('log lambda')\n",
    "plt.ylabel('log mean squared error')\n",
    "plt.title('Mean Squared Errors vs. Log Lambda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
