{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT5201 Assignment 2 Task 1: Document Clustering**  \n",
    "**Student ID: 31237223**  \n",
    "**Name: Darren Jer Shien Yee**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1: EM Derivation and Analysis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE for Complete Data**  \n",
    "\n",
    "$p(k,d) = p(k)p(d|k) = \\varphi k \\prod\\limits_{w\\in d} \\mu_{kw} = \\varphi k \\prod\\limits_{w\\in A} \\mu_{kw}^{c(w,d)}$  \n",
    "where $\\varphi k$ = prior probability of class, $\\mu_{kw}$ = proportion of word **w** in cluster **k** and ${c(w,d)}$ = occurence of word **w** in dictionary over any specific document **d**.  \n",
    "\n",
    "**MLE for Incomplete Data**  \n",
    "$p(d1,...,d_N) = \\prod\\limits_{n = 1}^N p(d_n) = \\prod\\limits_{n = 1}^N \\sum\\limits_{k = 1}^K p(d_n,z_n = k) = \\prod\\limits_{n = 1}^N \\sum\\limits_{k = 1}^K (\\varphi k \\prod\\limits_{w\\in A} \\mu_{kw}^{c(w,d)})$  \n",
    "\n",
    "**What is the main difference between MLE for Complete and Incomplete data and why is it hard to optimize)**  \n",
    "The main difference as we can see is the additional summation term $\\sum\\limits_{k = 1}^K$ within the MLE for incomplete data which makes it incredibly difficult to maximize. The existence of this term means that if we were to use solve this MLE using log likelihood, there would be a log term outside of this term $\\sum\\limits_{n = 1}^N\\log\\sum\\limits_{k = 1}^K$  which makes it hard to optimise as log of a sum is inherently a complicated mathematical concept to solve.  \n",
    "\n",
    "**High Level Description of EM algorithm to find MLE parameter estimates**  \n",
    "EM algorithm is an iterative algorithm that helps us solve the optimization problem faced in the explanation when trying to solve Maximum Likelihood Estimation with the existence of hidden variables $z$. It has two main components: E-step and M-step. During the E-step, we start by constructing a surrogate function $Q$ with a tight lower bound to $L(\\theta)$ (which is the loss function for the current $\\theta$ (current parameter estimation)). This forces the line constructed with $Q$ to be strictly lower than the loss function line. Then, using the Q function that we have obtained from the E-step, we perform M-Step by maximizing the Q function with respect to the current $\\theta$ estimation to optain a new set of $\\theta$ to be used in the next iteration's E-Step. Once converged, we should obtain a set of optimal parameters $\\theta$ which maximizes the MLE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2: E-Step and M-Step derivation for Document Clustering**  \n",
    "\n",
    "**E-Step for Document Clustering**  \n",
    "\n",
    "$\\forall n,\\forall k: r(z_{nk})$ based on current $\\theta^t$ where $r(z_{nk}) = p(z_n = k| d_n,\\theta^{old})$  \n",
    "\n",
    "**M-step for Document Clustering**  \n",
    "\n",
    "$Q(\\theta,\\theta^{old}): = \\sum\\limits_{n = 1}^N\\sum\\limits_{k = 1}^K p (z_{nk} = 1 | d_n, \\theta^{old})\\log p (z_{nk} = 1 | d_n, \\theta)$ **where $p(z_{nk} = 1)$ equivalent to $p(z_n = k)$**  \n",
    "$Q(\\theta,\\theta^{old}) = \\sum\\limits_{n = 1}^N\\sum\\limits_{k = 1}^K r(z_{nk}) log (\\varphi k \\prod\\limits_{w\\in A} \\mu_{kw}^{c(w,d)})$  \n",
    "$Q(\\theta,\\theta^{old}) = \\sum\\limits_{n = 1}^N\\sum\\limits_{k = 1}^K r(z_{nk}) (log \\varphi k + log\\prod\\limits_{w\\in A} \\mu_{kw}^{c(w,d)})$  \n",
    "$Q(\\theta,\\theta^{old}) = \\sum\\limits_{n = 1}^N\\sum\\limits_{k = 1}^K r(z_{nk}) (log \\varphi k + \\sum\\limits_{w\\in A} c (w,d_n)\\log\\mu_{kw})$  \n",
    "\n",
    "This allows us to obtain two subterms $\\varphi k$ (prior probability of k) and $\\mu_{kw}$ (probability of word in the dictionary) which can maximize their log likelihoods seperately with the update estimates below by setting their derivatives to 0. \n",
    "\n",
    "$\\varphi k = \\frac{N_k}{N}$ where $N_k = \\sum\\limits_{n=1}^N r(z_{nk})$  \n",
    "$\\mu_{kw} = \\frac{\\sum_{n=1}^N r (z_{nk})c(w,d_n)}{\\sum_{w'\\in A} \\sum_{n=1}^N r (z_{nk})c(w',d_n)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3: Load Task2a.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2373, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "with open('Task2a.txt', 'r') as file:\n",
    "    text = file.readlines()\n",
    "all([length == 2 for length in [len(line.split('\\t')) for line in text]])\n",
    "labels, articles = [line.split('\\t')[0].strip() for line in text], [line.split('\\t')[1].strip() for line in text]\n",
    "docs = pd.DataFrame(data = zip(labels,articles), columns=['label', 'article'])\n",
    "docs.label = docs.label.astype('category')\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 Feature Extraction as shown in Activity 4.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30288\n",
      "16806\n",
      "12153\n",
      "9724\n",
      "8094\n",
      "6986\n",
      "6182\n",
      "5562\n",
      "5063\n",
      "4593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2373, 8094)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "min_freq = 10\n",
    "feature_options = []\n",
    "for i in range(min_freq):\n",
    "    cv = CountVectorizer(lowercase=True,\n",
    "                     stop_words='english',\n",
    "                     min_df=i+1)\n",
    "    features = cv.fit_transform(raw_documents=articles)\n",
    "    feature_options += [features]\n",
    "    print(len(cv.get_feature_names_out()))\n",
    "\n",
    "features = feature_options[4] # going with min_freq = 5\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4: EM implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "class DocEM: \n",
    "    def __init__(self, K, tau_max=200, epsilon=0.01, random_state=None):\n",
    "        self.K = K               # number of GMM clusters\n",
    "        self.tau_max = tau_max   # max number of iterations\n",
    "        self.epsilon = epsilon  # minimum acceptable error rate\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'K': self.K,\n",
    "         'tau_max': self.tau_max,\n",
    "         'epsilon': self.epsilon,\n",
    "         'random_state': self.random_state}\n",
    "    \n",
    "    def __str__(self):\n",
    "        params = self.get_params()\n",
    "        return 'DocEM({0})'.format(','.join(['='.join([key, str(params[key])]) for key in params.keys()]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "                \n",
    "    def fit(self, X, verbose=False):\n",
    "        N = X.shape[0] # Number of Documents \n",
    "        W = X.shape[1] # Number of Words in the Dictionary\n",
    "        ## initialization:\n",
    "        self.Psi_hat_ = np.array([1/self.K] * self.K) # Prior probability of k (\\varphi k)\n",
    "        self.Nk_hat_ = self.Psi_hat_ * N     # Estimated number of documents in each clusters                                  \n",
    "        self.Mu_hat_ = np.random.rand(self.K, W)  # Estimated mean vectors for each clusters \n",
    "        self.Mu_hat_ = self.Mu_hat_ / np.sum(self.Mu_hat_, axis=1, keepdims=True)\n",
    "        r = np.zeros((N,self.K)) # Posterior matrix to store E-step values                                                 \n",
    "        \n",
    "        self.Mu_hat_historic_ = np.zeros(shape=(list(self.Mu_hat_.shape) + [self.tau_max])) # Mean value of clusters at each iteration\n",
    "        self.r_historic_ = np.zeros(shape=(N, self.K, self.tau_max)) # Posterior probability of each clusters at each iteration\n",
    "        terminate= False\n",
    "        tau = 0\n",
    "        # fitting loop - we iteratively take E and M steps until the termination criterion is met.\n",
    "        Mu_hat_old = self.Mu_hat_ # Mu_hat_old is used to store the last iteration value for mu_hat\n",
    "        while (not terminate):\n",
    "            if verbose: print('iteration {0}'.format(tau))\n",
    "            # E step:\n",
    "            for n in range (N):\n",
    "                for k in range(self.K):\n",
    "                    ## calculate the posterior based on the estimated means,covariance and cluster size:\n",
    "                    r[n, k] = np.log(self.Psi_hat_[k]) + np.sum(X[n, :] * np.log(self.Mu_hat_[k]))\n",
    "                self.r_historic_[:, :, tau] = r\n",
    "            # M step (note that we use the vectorised notation directly which is much better and faster than using a for loop):\n",
    "            # Set c to the max of x1....xN to ensure largest exponentiated term is exp(0) = 1 to prevent overflow\n",
    "            c = np.max(r, axis=1, keepdims=True)\n",
    "            # Utilising the formula given in the assignment specs\n",
    "            log_normalization = c + np.log(np.sum(np.exp(r - c), axis=1, keepdims=True))\n",
    "            r = np.exp(r - log_normalization)\n",
    "            for k in range(self.K):\n",
    "                mu_upper = np.dot(X, r[:, k])\n",
    "                mu_lower = np.sum(mu_upper,axis = 0)\n",
    "                print (mu_upper/mu_lower)\n",
    "                self.Psi_hat_[k] = sum(r[:,k])/N\n",
    "                print (self.Psi_hat_)\n",
    "                self.Mu_hat_ = (r.T @ X)/ self.Nk_hat_.reshape((-1,1))\n",
    "                \n",
    "            self.Mu_hat_historic_[:, :, tau] = self.Mu_hat_\n",
    "            tau +=1\n",
    "            # check termination condition\n",
    "            terminate = ((tau == self.tau_max) or np.allclose(self.Mu_hat_, Mu_hat_old, rtol= self.epsilon))\n",
    "            # record the means (neccessary for checking the termination criteria)\n",
    "            Mu_hat_old = self.Mu_hat_\n",
    "\n",
    "        self.Mu_hat_historic_ = self.Mu_hat_historic_[:, :, :tau]\n",
    "        self.r_historic_ = self.r_historic_[:, :, :tau]\n",
    "\n",
    "        if verbose: print(f'Converged in {tau} iterations')\n",
    "\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        N = x.shape[0]\n",
    "        r = np.zeros((N,self.K))\n",
    "        for k in range(self.K):\n",
    "            ## calculate the posterior based on the estimated means,covariance and cluster size:\n",
    "            r[:,k] = self.Psi_hat_[k] * multivariate_normal.pdf(x, mean=self.Mu_hat_[k], cov=self.Sigma_hat_[k])\n",
    "        r = r/r.sum(axis=1,keepdims=True) # normalization (to make sure sum(r)=1)\n",
    "        \n",
    "        return r\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'numpy.ndarray' and 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc_em \u001b[38;5;241m=\u001b[39m DocEM(K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m doc_em\u001b[38;5;241m.\u001b[39mfit(features, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[136], line 56\u001b[0m, in \u001b[0;36mDocEM.fit\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m     54\u001b[0m mu_upper \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, r[:, k])\n\u001b[0;32m     55\u001b[0m mu_lower \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(mu_upper,axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m (mu_upper\u001b[38;5;241m/\u001b[39mmu_lower)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPsi_hat_[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(r[:,k])\u001b[38;5;241m/\u001b[39mN\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPsi_hat_)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'numpy.ndarray' and 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "doc_em = DocEM(K=3, random_state=0)\n",
    "doc_em.fit(features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
